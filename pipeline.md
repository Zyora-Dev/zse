# ZSE Architecture Pipeline

> **Last Updated:** February 27, 2026  
> **Version:** 1.2.0

This document describes the actual code flow and architecture of ZSE (Z Server Engine).

---

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              USER ENTRY POINTS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CLI (zse chat/serve)  â”‚  Python API  â”‚  HTTP Server (/v1/chat/completions) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                  â”‚                          â”‚
             â–¼                  â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          .ZSE FORMAT LOADER                                  â”‚
â”‚                     zse/format/reader_v2.py                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ load_zse_model(path, cache_weights="auto")                          â”‚    â”‚
â”‚  â”‚ â€¢ Memory-mapped file access (fast cold start)                       â”‚    â”‚
â”‚  â”‚ â€¢ Embedded tokenizer + config (no network calls)                    â”‚    â”‚
â”‚  â”‚ â€¢ INT4 weights stored in packed format                              â”‚    â”‚
â”‚  â”‚ â€¢ Auto-converts to bnb format for CUDA inference                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QUANTIZED LINEAR LAYER                                  â”‚
â”‚                     QuantizedLinearZSE (reader_v2.py)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Storage: INT4 packed (0.5 bytes/param)                              â”‚    â”‚
â”‚  â”‚ Inference: bitsandbytes.matmul_4bit CUDA kernel                     â”‚    â”‚
â”‚  â”‚                                                                     â”‚    â”‚
â”‚  â”‚ On first forward():                                                 â”‚    â”‚
â”‚  â”‚   1. Dequantize ZSE INT4 â†’ FP16                                     â”‚    â”‚
â”‚  â”‚   2. Re-quantize with bnb (nf4 format)                              â”‚    â”‚
â”‚  â”‚   3. Use bnb.matmul_4bit for fast CUDA inference                    â”‚    â”‚
â”‚  â”‚                                                                     â”‚    â”‚
â”‚  â”‚ Result: ~60 tok/s for 7B, ~27 tok/s for 32B                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GENERATION ENGINE                                     â”‚
â”‚                     model.generate() with KV cache                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ .zse Format Data Flow

### 1. Conversion (One-Time)

```
zse convert Qwen/Qwen2.5-7B-Instruct -o qwen7b.zse

writer.py: convert_model()
    â”œâ”€â”€ Load model from HuggingFace
    â”œâ”€â”€ Quantize weights to INT4 (group_size=128)
    â”œâ”€â”€ Pack INT4 pairs into uint8
    â”œâ”€â”€ Serialize tokenizer files (base64)
    â”œâ”€â”€ Serialize HF config JSON
    â””â”€â”€ Write single .zse file with header
    
Output: qwen7b.zse (5.57 GB for 7B model)
```

### 2. Loading (.zse â†’ GPU)

```python
model, tokenizer, info = load_zse_model("qwen7b.zse")

reader_v2.py: load_zse_model()
    â”œâ”€â”€ Memory-map .zse file
    â”œâ”€â”€ Parse header (architecture, quantization, offsets)
    â”œâ”€â”€ Load tokenizer from embedded data (no network)
    â”œâ”€â”€ Load config from embedded JSON (no network)
    â”œâ”€â”€ Create model skeleton on meta device
    â”œâ”€â”€ Replace Linear layers with QuantizedLinearZSE
    â”œâ”€â”€ Load INT4 packed weights directly to GPU
    â””â”€â”€ Convert to bnb format (first forward or explicit)

VRAM: 5.9 GB for 7B, 20.9 GB for 32B
Load time: 9s for 7B, 24s for 32B
```

### 3. Inference (bnb.matmul_4bit)

```python
output = model.generate(input_ids, max_new_tokens=100)

QuantizedLinearZSE.forward(x):
    # First call: convert ZSE INT4 â†’ bnb nf4
    if self._bnb_weight is None:
        weight_fp16 = dequantize_int4_zse(self.weight_packed, ...)
        self._bnb_weight, self._bnb_quant_state = quantize_4bit(weight_fp16)
    
    # Fast CUDA kernel (0.018ms per 1024x1024 matmul)
    return bnb.matmul_4bit(x, self._bnb_weight.t(), quant_state=...)

Speed: 58.7 tok/s for 7B, 26.9 tok/s for 32B
```

---

## ğŸ“ Key Files

```
zse/format/
â”œâ”€â”€ writer.py           # convert_model() - HF â†’ .zse
â”‚   â”œâ”€â”€ quantize_to_int4()     # Group-wise INT4 quantization
â”‚   â””â”€â”€ ZSEWriter              # Serializes model + tokenizer + config
â”‚
â”œâ”€â”€ reader_v2.py        # load_zse_model() - .zse â†’ GPU
â”‚   â”œâ”€â”€ ZSEReaderV2            # Memory-mapped file access
â”‚   â”œâ”€â”€ QuantizedLinearZSE     # INT4 layer with bnb inference
â”‚   â”œâ”€â”€ convert_model_to_bnb() # Pre-convert all layers
â”‚   â””â”€â”€ cache_model_weights()  # Optional FP16 caching
â”‚
â””â”€â”€ spec.py             # ZSEHeader, TensorInfo, dtype enums
```

---

## ğŸ¯ Performance Summary

| Operation | 7B Model | 32B Model |
|-----------|----------|-----------|
| File size | 5.57 GB | 19.23 GB |
| Load time | 9.1s | 24.1s |
| VRAM usage | 5.9 GB | 20.9 GB |
| Inference speed | 58.7 tok/s | 26.9 tok/s |

### Why bnb.matmul_4bit?

Before v1.2.0, we had two options:
1. **Python dequantization**: 2.2 tok/s (32B) - unusable
2. **cache_weights (FP16)**: 32.7 tok/s but 82GB VRAM - too much

`bnb.matmul_4bit` gives us:
- **Fast CUDA kernel**: 26.9 tok/s for 32B
- **Low VRAM**: 20.9 GB (fits on 24GB GPUs)
- **Best of both worlds**: Speed + Memory efficiency

### 2. Server API Flow (HTTP Request)

```
HTTP Request:
    POST /v1/chat/completions
    {"model": "...", "messages": [...], "stream": true}

Code Path:
    app.py: chat_completions()                    # server/app.py:480
        â”œâ”€â”€ get_batching_state()
        â”‚   â””â”€â”€ If batching.enabled:
        â”‚       â””â”€â”€ batched_chat_completion()     # server/batching.py
        â”‚   â””â”€â”€ Else:
        â”‚       â””â”€â”€ _stream_chat_completion()     # Direct generation
        â”‚
        â””â”€â”€ _stream_chat_completion()             # server/app.py:630
            â”œâ”€â”€ Apply chat template
            â”œâ”€â”€ orch.generate(prompt, stream=True)
            â””â”€â”€ StreamingResponse(event_generator)
```

### 3. Batched Server Flow (High Throughput)

```
Enable Batching:
    POST /api/batching/enable

Batched Request Flow:
    POST /v1/chat/completions â†’ batching.py
    
    BatchingEngine                                # batching.py:92
        â”œâ”€â”€ start() â†’ Creates processing_loop task
        â”œâ”€â”€ generate() / generate_stream()
        â”‚   â”œâ”€â”€ Create BatchRequest
        â”‚   â””â”€â”€ Put in _request_queue
        â”‚
        â””â”€â”€ _processing_loop()                    # batching.py:282
            â”œâ”€â”€ _collect_batch()                  # Wait 50ms, collect requests
            â””â”€â”€ _process_batch()
                â”œâ”€â”€ _run_prefill()                # Process prompts with KV cache
                â”‚   â”œâ”€â”€ model(input_ids, use_cache=True)
                â”‚   â””â”€â”€ Store past_key_values in _kv_cache[request_id]
                â””â”€â”€ _run_decode()                 # Generate tokens
                    â”œâ”€â”€ model(input_ids, past_key_values=kv, use_cache=True)
                    â”œâ”€â”€ Update _kv_cache[request_id]
                    â””â”€â”€ Clean up cache when finished
```

---

## ğŸ“ File Structure & Responsibilities

```
zse/
â”œâ”€â”€ format/                      # â˜… .ZSE FORMAT (Main Feature)
â”‚   â”œâ”€â”€ writer.py                # convert_model() - HF â†’ .zse
â”‚   â”‚   â”œâ”€â”€ quantize_to_int4()       # Group-wise INT4 quantization
â”‚   â”‚   â””â”€â”€ ZSEWriter                # Serializes model + tokenizer + config
â”‚   â”‚
â”‚   â”œâ”€â”€ reader_v2.py             # â˜… load_zse_model() - .zse â†’ GPU
â”‚   â”‚   â”œâ”€â”€ ZSEReaderV2              # Memory-mapped file access
â”‚   â”‚   â”œâ”€â”€ QuantizedLinearZSE       # INT4 layer with bnb.matmul_4bit
â”‚   â”‚   â”œâ”€â”€ convert_model_to_bnb()   # Pre-convert layers
â”‚   â”‚   â””â”€â”€ cache_model_weights()    # Optional FP16 caching
â”‚   â”‚
â”‚   â””â”€â”€ spec.py                  # ZSEHeader, TensorInfo, dtype enums
â”‚
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Exports IntelligenceOrchestrator
â”‚   â”‚   â””â”€â”€ core.py              # Orchestrator (uses .zse format)
â”‚   â”‚
â”‚   â”œâ”€â”€ generation.py            # TextGenerator - Token-by-token generation
â”‚   â”œâ”€â”€ batching.py              # BatchingEngine - Async batching for server
â”‚   â””â”€â”€ kv_cache.py              # KV cache implementations
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ cli/
â”‚   â”‚   â””â”€â”€ main.py              # CLI: zse serve, zse convert, zse chat
â”‚   â”‚
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ app.py               # FastAPI server
â”‚       â””â”€â”€ batching.py          # Batched endpoints
â”‚
â””â”€â”€ core/
    â””â”€â”€ zattention/              # Custom attention (future)
```

---

## âœ… What's Working (v1.2.0)

| Component | File | Status | Notes |
|-----------|------|--------|-------|
| **.zse Writer** | format/writer.py | âœ… Active | INT4 quantization + embed tokenizer/config |
| **.zse Reader** | format/reader_v2.py | âœ… Active | Memory-mapped, direct GPU loading |
| **QuantizedLinearZSE** | format/reader_v2.py | âœ… Active | bnb.matmul_4bit inference |
| **IntelligenceOrchestrator** | orchestrator/core.py | âœ… Active | VRAM detection, auto-optimize |
| **TextGenerator** | generation.py | âœ… Active | KV cache generation |
| **FastAPI Server** | server/app.py | âœ… Active | OpenAI-compatible API |

## ğŸ¯ v1.2.0 Key Innovation

**Problem:** Python INT4 dequantization = 2.2 tok/s (unusable)

**Solution:** Use `bitsandbytes.matmul_4bit` CUDA kernel

```python
# QuantizedLinearZSE.forward() - v1.2.0
def forward(self, x):
    # Convert ZSE INT4 â†’ bnb format (first call only)
    if self._bnb_weight is None:
        self.convert_to_bnb()
    
    # Fast CUDA kernel
    return bnb.matmul_4bit(x, self._bnb_weight.t(), quant_state=self._bnb_quant_state)
```

**Result:**
- 32B: 2.2 â†’ **26.9 tok/s** (12x speedup)
- 7B: ~10 â†’ **58.7 tok/s** (6x speedup)
- VRAM unchanged: 5.9 GB (7B), 20.9 GB (32B)

---

## ğŸ”§ Configuration Options

### IntelligenceOrchestrator

```python
# Quantization modes
IntelligenceOrchestrator.min_memory(model)   # INT4 - lowest VRAM
IntelligenceOrchestrator.balanced(model)     # INT8 - balanced
IntelligenceOrchestrator.max_speed(model)    # FP16 - fastest
IntelligenceOrchestrator.auto(model)         # Auto-detect

# Multi-GPU
IntelligenceOrchestrator.multi_gpu(model, gpu_ids=[0,1])
```

### BatchingEngine

```python
BatchConfig(
    max_batch_size=32,          # Max concurrent requests
    max_tokens_per_batch=4096,  # Token limit
    batch_wait_timeout_ms=50,   # Wait time to form batches
    enable_cuda_graphs=False,   # CUDA graph optimization
)
```

### Server

```bash
# Start server
zse serve "Qwen/Qwen2.5-14B-Instruct" --host 0.0.0.0 --port 8000

# Enable batching (runtime)
curl -X POST http://localhost:8000/api/batching/enable
```

---

## ğŸ“ˆ Request Lifecycle Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           REQUEST LIFECYCLE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[User Request]
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenize    â”‚  prompt â†’ input_ids
â”‚ Prompt      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PREFILL     â”‚  Process full prompt, generate first token
â”‚ Phase       â”‚  Returns: logits + KV cache
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DECODE      â”‚  Loop until stop condition:
â”‚ Loop        â”‚    1. Pass only NEW token + KV cache
â”‚             â”‚    2. Get logits for next token
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ model(new_token, past_key_values=kv)   â”‚
â”‚  â”‚ â†’ logits, updated_kv_cache             â”‚
â”‚  â”‚ â†’ sample(logits) â†’ next_token          â”‚
â”‚  â”‚ â†’ yield StreamChunk(next_token)        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STOP        â”‚  EOS token / max_tokens / stop_sequence
â”‚ Condition   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
[Complete Response]
```

---

## ğŸ§ª Testing Code Paths

```python
# Test 1: Python API (TextGenerator path)
from zse.engine.orchestrator import IntelligenceOrchestrator

orch = IntelligenceOrchestrator.auto("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
orch.load()
for chunk in orch.generate("Say hello", max_tokens=20, stream=True):
    print(chunk, end="", flush=True)

# Test 2: Server (start then curl)
# Terminal 1:
# zse serve "TinyLlama/TinyLlama-1.1B-Chat-v1.0" --port 8000

# Terminal 2:
# curl http://localhost:8000/v1/chat/completions \
#   -d '{"model":"default","messages":[{"role":"user","content":"Hello"}]}'

# Test 3: Enable batching
# curl -X POST http://localhost:8000/api/batching/enable
# Then send concurrent requests
```

---

## ğŸ“ Version History

| Version | Change |
|---------|--------|
| 0.1.4 | Fixed KV cache in TextGenerator and BatchingEngine |
| 0.1.3 | Added multi-GPU support |
| 0.1.2 | Added server batching endpoints |
| 0.1.1 | Initial release with INT4/INT8/FP16 |
