(()=>{var e={};e.id=761,e.ids=[761],e.modules={7849:e=>{"use strict";e.exports=require("next/dist/client/components/action-async-storage.external")},2934:e=>{"use strict";e.exports=require("next/dist/client/components/action-async-storage.external.js")},5403:e=>{"use strict";e.exports=require("next/dist/client/components/request-async-storage.external")},4580:e=>{"use strict";e.exports=require("next/dist/client/components/request-async-storage.external.js")},4749:e=>{"use strict";e.exports=require("next/dist/client/components/static-generation-async-storage.external")},5869:e=>{"use strict";e.exports=require("next/dist/client/components/static-generation-async-storage.external.js")},399:e=>{"use strict";e.exports=require("next/dist/compiled/next-server/app-page.runtime.prod.js")},5348:(e,t,s)=>{"use strict";s.r(t),s.d(t,{GlobalError:()=>l.a,__next_app__:()=>x,originalPathname:()=>p,pages:()=>c,routeModule:()=>m,tree:()=>d});var a=s(482),r=s(9108),i=s(2563),l=s.n(i),n=s(8300),o={};for(let e in n)0>["default","tree","pages","GlobalError","originalPathname","__next_app__","routeModule"].indexOf(e)&&(o[e]=()=>n[e]);s.d(t,o);let d=["",{children:["docs",{children:["gguf",{children:["__PAGE__",{},{page:[()=>Promise.resolve().then(s.bind(s,9604)),"/Users/redfoxhotels/zse/website/src/app/docs/gguf/page.tsx"]}]},{}]},{layout:[()=>Promise.resolve().then(s.bind(s,9231)),"/Users/redfoxhotels/zse/website/src/app/docs/layout.tsx"]}]},{layout:[()=>Promise.resolve().then(s.bind(s,7633)),"/Users/redfoxhotels/zse/website/src/app/layout.tsx"],"not-found":[()=>Promise.resolve().then(s.t.bind(s,9361,23)),"next/dist/client/components/not-found-error"]}],c=["/Users/redfoxhotels/zse/website/src/app/docs/gguf/page.tsx"],p="/docs/gguf/page",x={require:s,loadChunk:()=>Promise.resolve()},m=new a.AppPageRouteModule({definition:{kind:r.x.APP_PAGE,page:"/docs/gguf/page",pathname:"/docs/gguf",bundlePath:"",filename:"",appPaths:[]},userland:{loaderTree:d}})},3329:(e,t,s)=>{Promise.resolve().then(s.bind(s,8210))},8210:(e,t,s)=>{"use strict";s.r(t),s.d(t,{default:()=>d});var a=s(5344),r=s(1499),i=s(196),l=s(1812),n=s(9039);let o=[{id:"overview",title:"Overview",level:2},{id:"supported-formats",title:"Supported Formats",level:2},{id:"installation",title:"Installation",level:2},{id:"usage",title:"Usage",level:2},{id:"gpu-offloading",title:"GPU Offloading",level:2},{id:"zse-vs-gguf",title:".zse vs GGUF",level:2}];function d(){return(0,a.jsxs)("div",{className:"flex",children:[(0,a.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[a.jsx(r.lv,{title:"GGUF Compatibility",description:"Run GGUF models via llama.cpp backend for compatibility with existing model files.",badge:"Feature"}),(0,a.jsxs)(r.Je,{id:"overview",title:"Overview",children:[a.jsx("p",{className:"mb-4",children:"ZSE includes full support for GGUF models via the llama-cpp-python backend. This lets you run existing GGUF models from Hugging Face or other sources while maintaining API compatibility with the rest of ZSE."}),(0,a.jsxs)(n.gy,{columns:3,children:[a.jsx(n.Zb,{title:"GGUF v2/v3",description:"Full support for modern GGUF format versions"}),a.jsx(n.Zb,{title:"All Quant Types",description:"Q4_K_M, Q5_K_M, Q8_0, and more"}),a.jsx(n.Zb,{title:"GPU Offloading",description:"Configurable layer offloading to GPU"})]}),a.jsx(n.VS,{features:["Parse GGUF v2/v3 format metadata","Support all GGML quantization types","Streaming and non-streaming generation","Chat completion support","GPU layer offloading configuration","Seamless integration with ZSE server"]})]}),(0,a.jsxs)(r.Je,{id:"supported-formats",title:"Supported Formats",children:[a.jsx("p",{className:"mb-4",children:"ZSE supports all standard GGML quantization types found in GGUF files:"}),a.jsx("div",{className:"overflow-x-auto",children:(0,a.jsxs)("table",{className:"w-full text-sm",children:[a.jsx("thead",{children:(0,a.jsxs)("tr",{className:"border-b border-white/10",children:[a.jsx("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Format"}),a.jsx("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Bits"}),a.jsx("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Use Case"})]})}),(0,a.jsxs)("tbody",{children:[(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 font-mono text-lime",children:"Q4_K_M"}),a.jsx("td",{className:"py-3 px-4 text-white",children:"4-bit"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"Best balance of size/quality"})]}),(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 font-mono text-lime",children:"Q5_K_M"}),a.jsx("td",{className:"py-3 px-4 text-white",children:"5-bit"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"Higher quality, slightly larger"})]}),(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 font-mono text-lime",children:"Q8_0"}),a.jsx("td",{className:"py-3 px-4 text-white",children:"8-bit"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"Near-lossless quality"})]}),(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 font-mono text-lime",children:"Q2_K"}),a.jsx("td",{className:"py-3 px-4 text-white",children:"2-bit"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"Maximum compression"})]}),(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 font-mono text-lime",children:"Q6_K"}),a.jsx("td",{className:"py-3 px-4 text-white",children:"6-bit"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"High quality"})]})]})]})})]}),(0,a.jsxs)(r.Je,{id:"installation",title:"Installation",children:[a.jsx(r.KU,{id:"cpu-only",title:"CPU Only",children:a.jsx(i.d,{language:"bash",code:"pip install llama-cpp-python"})}),a.jsx(r.KU,{id:"with-cuda",title:"With CUDA (Recommended)",children:a.jsx(i.d,{language:"bash",code:'CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python'})}),a.jsx(r.KU,{id:"with-metal",title:"With Metal (macOS)",children:a.jsx(i.d,{language:"bash",code:'CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python'})}),a.jsx(l.U,{type:"info",children:"GPU acceleration is highly recommended. CPU-only inference is 10-50x slower."})]}),(0,a.jsxs)(r.Je,{id:"usage",title:"Usage",children:[a.jsx(r.KU,{id:"python-api",title:"Python API",children:a.jsx(i.d,{language:"python",code:`from zse.gguf import GGUFWrapper, is_gguf_file

# Check if file is GGUF format
if is_gguf_file("model-Q4_K_M.gguf"):
    # Create wrapper (matches IntelligenceOrchestrator API)
    wrapper = GGUFWrapper("model-Q4_K_M.gguf")
    wrapper.load()
    
    # Streaming generation
    for text in wrapper.generate("Hello, how are you?"):
        print(text, end="")

    # Chat completion
    response = wrapper.chat([
        {"role": "user", "content": "Write a haiku about coding"}
    ])
    print(response)`})}),a.jsx(r.KU,{id:"cli",title:"CLI",children:a.jsx(i.d,{language:"bash",code:`# Auto-detect GGUF format and serve
zse serve model-Q4_K_M.gguf

# Show GGUF metadata
zse info model-Q4_K_M.gguf

# Run inference directly
zse infer model-Q4_K_M.gguf --prompt "Hello, world!"`})}),a.jsx(r.KU,{id:"reading-metadata",title:"Reading Metadata",children:a.jsx(i.d,{language:"python",code:`from zse.gguf import GGUFReader

reader = GGUFReader("model-Q4_K_M.gguf")
metadata = reader.read_metadata()

print(f"Architecture: {metadata['architecture']}")
print(f"Context Length: {metadata['context_length']}")
print(f"Layers: {metadata['num_layers']}")
print(f"Quantization: {metadata['quantization_type']}")`})})]}),(0,a.jsxs)(r.Je,{id:"gpu-offloading",title:"GPU Offloading",children:[a.jsx("p",{className:"mb-4",children:"Configure how many layers to offload to GPU for faster inference:"}),a.jsx(i.d,{language:"python",code:`from zse.gguf import GGUFWrapper

# Offload all layers to GPU (fastest, requires most VRAM)
wrapper = GGUFWrapper("model.gguf", n_gpu_layers=-1)

# Offload first 20 layers to GPU
wrapper = GGUFWrapper("model.gguf", n_gpu_layers=20)

# CPU only (no GPU offloading)
wrapper = GGUFWrapper("model.gguf", n_gpu_layers=0)

# Auto-detect optimal layers based on available VRAM
wrapper = GGUFWrapper("model.gguf")  # Default behavior`}),a.jsx(l.U,{type:"warning",children:"The more layers offloaded to GPU, the faster inference will be. However, you need sufficient VRAM. If you run out of memory, reduce n_gpu_layers."})]}),(0,a.jsxs)(r.Je,{id:"zse-vs-gguf",title:".zse vs GGUF",children:[(0,a.jsxs)("p",{className:"mb-4",children:["While GGUF is widely supported, native ",a.jsx(i.Z,{children:".zse"})," format offers significant advantages:"]}),a.jsx("div",{className:"overflow-x-auto",children:(0,a.jsxs)("table",{className:"w-full text-sm",children:[a.jsx("thead",{children:(0,a.jsxs)("tr",{className:"border-b border-white/10",children:[a.jsx("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Feature"}),a.jsx("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:".zse"}),a.jsx("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"GGUF"})]})}),(0,a.jsxs)("tbody",{children:[(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 text-white",children:"Memory Allocation"}),a.jsx("td",{className:"py-3 px-4 text-lime",children:"Streaming (on-demand)"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"Static (all at once)"})]}),(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 text-white",children:"Cold Start"}),a.jsx("td",{className:"py-3 px-4 text-lime",children:"3.9s (7B model)"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"10-30s typical"})]}),(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 text-white",children:"Memory Efficiency"}),a.jsx("td",{className:"py-3 px-4 text-lime",children:"Load only needed layers"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"Full model in RAM+VRAM"})]}),(0,a.jsxs)("tr",{className:"border-b border-white/5",children:[a.jsx("td",{className:"py-3 px-4 text-white",children:"Quantization"}),a.jsx("td",{className:"py-3 px-4 text-lime",children:"INT4 @ full precision"}),a.jsx("td",{className:"py-3 px-4 text-gray-400",children:"Various (Q4_K_M, etc.)"})]})]})]})}),a.jsx(l.U,{type:"info",children:"Use GGUF for compatibility with existing model files. Convert to .zse for optimal performance with ZSE's streaming inference engine."}),a.jsx(i.d,{language:"bash",code:`# Convert GGUF to .zse for better performance
zse convert model-Q4_K_M.gguf -o model.zse`})]}),a.jsx(r.KO,{prev:{href:"/docs/multi-gpu",title:"Multi-GPU"},next:{href:"/docs/api/cli",title:"CLI Commands"}})]}),a.jsx(r.o5,{items:o})]})}},9039:(e,t,s)=>{"use strict";s.d(t,{Rg:()=>n,VS:()=>o,Zb:()=>d,gy:()=>c});var a=s(5344),r=s(1912),i=s(2312),l=s(1453);function n({steps:e}){return a.jsx("div",{className:"my-6 space-y-0",children:e.map((t,s)=>(0,a.jsxs)(r.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*s},className:"relative pl-8 pb-8 last:pb-0",children:[s<e.length-1&&a.jsx("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),a.jsx("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:a.jsx("span",{className:"text-xs font-bold text-lime",children:s+1})}),(0,a.jsxs)("div",{children:[a.jsx("h4",{className:"text-base font-semibold text-white mb-1",children:t.title}),t.description&&a.jsx("p",{className:"text-sm text-white/50 mb-3",children:t.description}),t.code&&a.jsx("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:a.jsx("code",{className:"text-sm text-lime/90 font-mono",children:t.code})}),t.content&&a.jsx("div",{className:"text-sm text-white/70",children:t.content})]})]},s))})}function o({features:e}){return a.jsx("ul",{className:"my-4 space-y-2",children:e.map((e,t)=>(0,a.jsxs)(r.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*t},className:"flex items-start gap-2",children:[a.jsx(i.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),a.jsx("span",{className:"text-sm text-white/70",children:e})]},t))})}function d({title:e,description:t,icon:s,href:i,children:n}){let o=i?"a":"div";return a.jsx(r.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:i?{y:-2}:void 0,children:(0,a.jsxs)(o,{...i?{href:i,className:"block"}:{},className:(0,l.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",i&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[s&&a.jsx("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:a.jsx(s,{className:"w-4 h-4 text-lime"})}),a.jsx("h4",{className:"text-base font-semibold text-white mb-1",children:e}),t&&a.jsx("p",{className:"text-sm text-white/50",children:t}),n]})})}function c({children:e,columns:t=2}){return a.jsx("div",{className:(0,l.cn)("grid gap-4 my-6",2===t&&"md:grid-cols-2",3===t&&"md:grid-cols-3"),children:e})}},9604:(e,t,s)=>{"use strict";s.r(t),s.d(t,{$$typeof:()=>i,__esModule:()=>r,default:()=>l});let a=(0,s(6843).createProxy)(String.raw`/Users/redfoxhotels/zse/website/src/app/docs/gguf/page.tsx`),{__esModule:r,$$typeof:i}=a,l=a.default}};var t=require("../../../webpack-runtime.js");t.C(e);var s=e=>t(t.s=e),a=t.X(0,[638,498,697,224,782,883],()=>s(5348));module.exports=a})();