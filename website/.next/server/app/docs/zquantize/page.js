(()=>{var e={};e.id=801,e.ids=[801],e.modules={7849:e=>{"use strict";e.exports=require("next/dist/client/components/action-async-storage.external")},2934:e=>{"use strict";e.exports=require("next/dist/client/components/action-async-storage.external.js")},5403:e=>{"use strict";e.exports=require("next/dist/client/components/request-async-storage.external")},4580:e=>{"use strict";e.exports=require("next/dist/client/components/request-async-storage.external.js")},4749:e=>{"use strict";e.exports=require("next/dist/client/components/static-generation-async-storage.external")},5869:e=>{"use strict";e.exports=require("next/dist/client/components/static-generation-async-storage.external.js")},399:e=>{"use strict";e.exports=require("next/dist/compiled/next-server/app-page.runtime.prod.js")},4980:(e,t,i)=>{"use strict";i.r(t),i.d(t,{GlobalError:()=>n.a,__next_app__:()=>x,originalPathname:()=>m,pages:()=>c,routeModule:()=>p,tree:()=>d});var s=i(482),a=i(9108),r=i(2563),n=i.n(r),l=i(8300),o={};for(let e in l)0>["default","tree","pages","GlobalError","originalPathname","__next_app__","routeModule"].indexOf(e)&&(o[e]=()=>l[e]);i.d(t,o);let d=["",{children:["docs",{children:["zquantize",{children:["__PAGE__",{},{page:[()=>Promise.resolve().then(i.bind(i,9684)),"/Users/redfoxhotels/zse/website/src/app/docs/zquantize/page.tsx"]}]},{}]},{layout:[()=>Promise.resolve().then(i.bind(i,9231)),"/Users/redfoxhotels/zse/website/src/app/docs/layout.tsx"]}]},{layout:[()=>Promise.resolve().then(i.bind(i,7633)),"/Users/redfoxhotels/zse/website/src/app/layout.tsx"],"not-found":[()=>Promise.resolve().then(i.t.bind(i,9361,23)),"next/dist/client/components/not-found-error"]}],c=["/Users/redfoxhotels/zse/website/src/app/docs/zquantize/page.tsx"],m="/docs/zquantize/page",x={require:i,loadChunk:()=>Promise.resolve()},p=new s.AppPageRouteModule({definition:{kind:a.x.APP_PAGE,page:"/docs/zquantize/page",pathname:"/docs/zquantize",bundlePath:"",filename:"",appPaths:[]},userland:{loaderTree:d}})},6673:(e,t,i)=>{Promise.resolve().then(i.bind(i,7096))},7096:(e,t,i)=>{"use strict";i.r(t),i.d(t,{default:()=>d});var s=i(5344),a=i(1499),r=i(196),n=i(1812),l=i(9039);let o=[{id:"overview",title:"Overview",level:2},{id:"quick-start",title:"Quick Start",level:2},{id:"quantization-types",title:"Quantization Types",level:2},{id:"advanced-options",title:"Advanced Options",level:2},{id:"batch-conversion",title:"Batch Conversion",level:2},{id:"quality-validation",title:"Quality Validation",level:2}];function d(){return(0,s.jsxs)("div",{className:"flex",children:[(0,s.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[s.jsx(a.lv,{title:"zQuantize",description:"Convert models to optimized .zse format with extreme compression and minimal quality loss.",badge:"Feature"}),(0,s.jsxs)(a.Je,{id:"overview",title:"Overview",children:[(0,s.jsxs)("p",{className:"mb-4",children:[s.jsx(r.Z,{children:"zQuantize"})," converts transformer models from HuggingFace, safetensors, or GGUF to ZSE's native format with configurable quantization."]}),s.jsx(l.VS,{features:["4-bit and 8-bit quantization with calibration","NF4 (NormalFloat4) for best quality","Group quantization for accuracy preservation","Mixed precision for critical layers","CUDA-accelerated conversion"]})]}),(0,s.jsxs)(a.Je,{id:"quick-start",title:"Quick Start",children:[s.jsx(l.Rg,{steps:[{title:"Convert a model",description:"Run the convert command with default settings (NF4)",code:"zse convert Qwen/Qwen2.5-7B-Instruct -o qwen-7b.zse"},{title:"Verify the conversion",description:"Check model info and size",code:"zse info qwen-7b.zse"},{title:"Test the model",description:"Run a quick inference test",code:'zse chat qwen-7b.zse -p "Hello, how are you?"'}]}),s.jsx(n.U,{type:"info",children:"Conversion time depends on model size and GPU availability. A 7B model takes ~2 minutes on GPU or ~15 minutes on CPU."})]}),(0,s.jsxs)(a.Je,{id:"quantization-types",title:"Quantization Types",children:[s.jsx("p",{className:"mb-4",children:"Choose the right quantization type for your memory/quality tradeoff:"}),(0,s.jsxs)(l.gy,{columns:2,children:[s.jsx(l.Zb,{title:"NF4 (Default)",description:"Best quality 4-bit — normalized float distribution"}),s.jsx(l.Zb,{title:"INT4",description:"Standard 4-bit — fastest conversion speed"}),s.jsx(l.Zb,{title:"INT8",description:"8-bit integer — higher quality, larger files"}),s.jsx(l.Zb,{title:"FP16",description:"Half precision — maximum quality, largest files"})]}),(0,s.jsxs)(a.KU,{id:"nf4",title:"NF4 Quantization",children:[s.jsx("p",{className:"mb-2",children:"NormalFloat4 is optimized for the weight distribution of neural networks:"}),s.jsx(r.d,{language:"bash",code:"zse convert model-id -o model.zse --quant nf4"}),s.jsx(l.VS,{features:["Asymmetric quantization grid","Optimal for normally-distributed weights","Industry-leading 4-bit quality","Default for most models"]})]}),(0,s.jsxs)(a.KU,{id:"int4",title:"INT4 Quantization",children:[s.jsx("p",{className:"mb-2",children:"Standard symmetric 4-bit integer quantization:"}),s.jsx(r.d,{language:"bash",code:"zse convert model-id -o model.zse --quant int4"}),s.jsx(l.VS,{features:["Symmetric quantization grid","Faster conversion than NF4","Compatible with more hardware","Slightly lower quality than NF4"]})]}),(0,s.jsxs)(a.KU,{id:"int8",title:"INT8 Quantization",children:[s.jsx("p",{className:"mb-2",children:"8-bit quantization for higher quality:"}),s.jsx(r.d,{language:"bash",code:"zse convert model-id -o model.zse --quant int8"})]}),(0,s.jsxs)(a.KU,{id:"fp16",title:"FP16 (No Quantization)",children:[s.jsx("p",{className:"mb-2",children:"Convert without quantization — useful for fine-tuning or maximum quality:"}),s.jsx(r.d,{language:"bash",code:"zse convert model-id -o model.zse --quant fp16"})]}),s.jsx("div",{className:"overflow-x-auto my-6",children:(0,s.jsxs)("table",{className:"w-full text-sm",children:[s.jsx("thead",{children:(0,s.jsxs)("tr",{className:"border-b border-white/10",children:[s.jsx("th",{className:"text-left py-2 pr-4 text-white/50 font-medium",children:"Type"}),s.jsx("th",{className:"text-right py-2 px-4 text-white/50 font-medium",children:"7B Size"}),s.jsx("th",{className:"text-right py-2 px-4 text-white/50 font-medium",children:"Quality"}),s.jsx("th",{className:"text-right py-2 pl-4 text-white/50 font-medium",children:"Speed"})]})}),(0,s.jsxs)("tbody",{className:"text-white/70",children:[(0,s.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[s.jsx("td",{className:"py-2 pr-4",children:s.jsx(r.Z,{children:"nf4"})}),s.jsx("td",{className:"text-right py-2 px-4",children:"4.2 GB"}),s.jsx("td",{className:"text-right py-2 px-4 text-lime",children:"★★★★☆"}),s.jsx("td",{className:"text-right py-2 pl-4",children:"★★★★☆"})]}),(0,s.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[s.jsx("td",{className:"py-2 pr-4",children:s.jsx(r.Z,{children:"int4"})}),s.jsx("td",{className:"text-right py-2 px-4",children:"4.0 GB"}),s.jsx("td",{className:"text-right py-2 px-4",children:"★★★☆☆"}),s.jsx("td",{className:"text-right py-2 pl-4 text-lime",children:"★★★★★"})]}),(0,s.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[s.jsx("td",{className:"py-2 pr-4",children:s.jsx(r.Z,{children:"int8"})}),s.jsx("td",{className:"text-right py-2 px-4",children:"7.5 GB"}),s.jsx("td",{className:"text-right py-2 px-4 text-lime",children:"★★★★★"}),s.jsx("td",{className:"text-right py-2 pl-4",children:"★★★☆☆"})]}),(0,s.jsxs)("tr",{children:[s.jsx("td",{className:"py-2 pr-4",children:s.jsx(r.Z,{children:"fp16"})}),s.jsx("td",{className:"text-right py-2 px-4",children:"14 GB"}),s.jsx("td",{className:"text-right py-2 px-4 text-lime",children:"★★★★★"}),s.jsx("td",{className:"text-right py-2 pl-4",children:"★★☆☆☆"})]})]})]})})]}),(0,s.jsxs)(a.Je,{id:"advanced-options",title:"Advanced Options",children:[(0,s.jsxs)(a.KU,{id:"group-size",title:"Group Size",children:[s.jsx("p",{className:"mb-2",children:"Control quantization granularity. Smaller groups = better quality, larger files:"}),s.jsx(r.d,{language:"bash",code:`# Default group size (128)
zse convert model -o model.zse --quant nf4

# Smaller groups for better quality
zse convert model -o model.zse --quant nf4 --group-size 64

# Larger groups for smaller files
zse convert model -o model.zse --quant nf4 --group-size 256`})]}),(0,s.jsxs)(a.KU,{id:"calibration",title:"Calibration Dataset",children:[s.jsx("p",{className:"mb-2",children:"Use calibration data for optimal quantization ranges:"}),s.jsx(r.d,{language:"bash",code:`# Use built-in calibration (default)
zse convert model -o model.zse --calibrate

# Use custom calibration data
zse convert model -o model.zse --calibrate-data ./prompts.txt

# Skip calibration (faster, slightly lower quality)
zse convert model -o model.zse --no-calibrate`}),s.jsx(n.U,{type:"tip",children:"Custom calibration with domain-specific data can improve quality for specialized tasks."})]}),(0,s.jsxs)(a.KU,{id:"mixed-precision",title:"Mixed Precision",children:[s.jsx("p",{className:"mb-2",children:"Keep critical layers at higher precision:"}),s.jsx(r.d,{language:"bash",code:`# Keep embedding and output layers at FP16
zse convert model -o model.zse --quant nf4 --mixed-precision

# Keep specific layers at higher precision
zse convert model -o model.zse --quant nf4 --keep-fp16 "embed,lm_head"`})]})]}),(0,s.jsxs)(a.Je,{id:"batch-conversion",title:"Batch Conversion",children:[s.jsx("p",{className:"mb-4",children:"Convert multiple models in a script:"}),s.jsx(r.d,{language:"bash",code:`#!/bin/bash
MODELS=(
  "Qwen/Qwen2.5-7B-Instruct"
  "Qwen/Qwen2.5-14B-Instruct"
  "meta-llama/Llama-3.1-8B-Instruct"
)

for model in "\${MODELS[@]}"; do
  name=$(basename "$model" | tr '[:upper:]' '[:lower:]')
  zse convert "$model" -o "./models/$name.zse" --quant nf4
done`}),s.jsx("p",{className:"mt-4 mb-2",children:"Python API for programmatic conversion:"}),s.jsx(r.d,{language:"python",code:`from zllm_zse import convert_model

# Convert with options
convert_model(
    source="Qwen/Qwen2.5-7B-Instruct",
    output="qwen-7b.zse",
    quant="nf4",
    group_size=128,
    calibrate=True
)

# Convert multiple models
models = ["model-a", "model-b", "model-c"]
for model in models:
    convert_model(model, f"{model}.zse")`})]}),(0,s.jsxs)(a.Je,{id:"quality-validation",title:"Quality Validation",children:[s.jsx("p",{className:"mb-4",children:"Verify quantized model quality with built-in benchmarks:"}),s.jsx(r.d,{language:"bash",code:`# Run perplexity benchmark
zse benchmark qwen-7b.zse --metric perplexity

# Compare with original
zse benchmark qwen-7b.zse --compare Qwen/Qwen2.5-7B-Instruct

# Full evaluation suite
zse benchmark qwen-7b.zse --eval mmlu,hellaswag,arc`}),s.jsx("p",{className:"mt-4",children:"Example output:"}),s.jsx(r.d,{language:"text",code:`┌─────────────────────────────────────────────────────────┐
│ Model: qwen-7b.zse (NF4, 4.2 GB)                        │
├─────────────────────────────────────────────────────────┤
│ Perplexity:     5.42 (original: 5.38, Δ +0.7%)         │
│ MMLU:           64.2% (original: 64.8%, Δ -0.6%)       │
│ HellaSwag:      78.1% (original: 78.9%, Δ -0.8%)       │
│ ARC-Challenge:  52.3% (original: 53.1%, Δ -0.8%)       │
└─────────────────────────────────────────────────────────┘`}),s.jsx(n.U,{type:"success",children:"Less than 1% quality loss is typical with NF4 quantization."})]}),s.jsx(a.KO,{prev:{title:"Memory Management",href:"/docs/memory"},next:{title:"zServe",href:"/docs/zserve"}})]}),s.jsx(a.o5,{items:o})]})}},9039:(e,t,i)=>{"use strict";i.d(t,{Rg:()=>l,VS:()=>o,Zb:()=>d,gy:()=>c});var s=i(5344),a=i(1912),r=i(2312),n=i(1453);function l({steps:e}){return s.jsx("div",{className:"my-6 space-y-0",children:e.map((t,i)=>(0,s.jsxs)(a.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*i},className:"relative pl-8 pb-8 last:pb-0",children:[i<e.length-1&&s.jsx("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),s.jsx("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:s.jsx("span",{className:"text-xs font-bold text-lime",children:i+1})}),(0,s.jsxs)("div",{children:[s.jsx("h4",{className:"text-base font-semibold text-white mb-1",children:t.title}),t.description&&s.jsx("p",{className:"text-sm text-white/50 mb-3",children:t.description}),t.code&&s.jsx("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:s.jsx("code",{className:"text-sm text-lime/90 font-mono",children:t.code})}),t.content&&s.jsx("div",{className:"text-sm text-white/70",children:t.content})]})]},i))})}function o({features:e}){return s.jsx("ul",{className:"my-4 space-y-2",children:e.map((e,t)=>(0,s.jsxs)(a.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*t},className:"flex items-start gap-2",children:[s.jsx(r.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),s.jsx("span",{className:"text-sm text-white/70",children:e})]},t))})}function d({title:e,description:t,icon:i,href:r,children:l}){let o=r?"a":"div";return s.jsx(a.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:r?{y:-2}:void 0,children:(0,s.jsxs)(o,{...r?{href:r,className:"block"}:{},className:(0,n.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",r&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[i&&s.jsx("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:s.jsx(i,{className:"w-4 h-4 text-lime"})}),s.jsx("h4",{className:"text-base font-semibold text-white mb-1",children:e}),t&&s.jsx("p",{className:"text-sm text-white/50",children:t}),l]})})}function c({children:e,columns:t=2}){return s.jsx("div",{className:(0,n.cn)("grid gap-4 my-6",2===t&&"md:grid-cols-2",3===t&&"md:grid-cols-3"),children:e})}},9684:(e,t,i)=>{"use strict";i.r(t),i.d(t,{$$typeof:()=>r,__esModule:()=>a,default:()=>n});let s=(0,i(6843).createProxy)(String.raw`/Users/redfoxhotels/zse/website/src/app/docs/zquantize/page.tsx`),{__esModule:a,$$typeof:r}=s,n=s.default}};var t=require("../../../webpack-runtime.js");t.C(e);var i=e=>t(t.s=e),s=t.X(0,[638,498,697,224,782,883],()=>i(4980));module.exports=s})();