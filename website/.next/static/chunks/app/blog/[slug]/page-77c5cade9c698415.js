(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[308],{2814:function(e,n,t){Promise.resolve().then(t.bind(t,4978))},7461:function(e,n,t){"use strict";t.d(n,{Z:function(){return r}});var s=t(4090),a={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};/**
 * @license lucide-react v0.344.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */let o=e=>e.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase().trim(),r=(e,n)=>{let t=(0,s.forwardRef)((t,r)=>{let{color:i="currentColor",size:l=24,strokeWidth:m=2,absoluteStrokeWidth:c,className:d="",children:u,...h}=t;return(0,s.createElement)("svg",{ref:r,...a,width:l,height:l,stroke:i,strokeWidth:c?24*Number(m)/Number(l):m,className:["lucide","lucide-".concat(o(e)),d].join(" "),...h},[...n.map(e=>{let[n,t]=e;return(0,s.createElement)(n,t)}),...Array.isArray(u)?u:[u]])});return t.displayName="".concat(e),t}},7907:function(e,n,t){"use strict";var s=t(5313);t.o(s,"useParams")&&t.d(n,{useParams:function(){return s.useParams}}),t.o(s,"usePathname")&&t.d(n,{usePathname:function(){return s.usePathname}})},4978:function(e,n,t){"use strict";t.r(n),t.d(n,{default:function(){return c}});var s=t(3827),a=t(7907),o=t(8792),r=t(5293),i=t(1604),l=t(2169);/**
 * @license lucide-react v0.344.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */let m=(0,t(7461).Z)("ArrowLeft",[["path",{d:"m12 19-7-7 7-7",key:"1l729n"}],["path",{d:"M19 12H5",key:"x3x0zl"}]]);function c(){let e=(0,a.useParams)().slug,n=(0,i.zl)(e);if(!n)return(0,s.jsx)("div",{className:"min-h-screen bg-black pt-24 pb-16",children:(0,s.jsxs)("div",{className:"max-w-3xl mx-auto px-6 text-center",children:[(0,s.jsx)("h1",{className:"text-2xl font-bold text-white mb-4",children:"Post not found"}),(0,s.jsx)(o.default,{href:"/blog",className:"text-lime hover:underline",children:"← Back to blog"})]})});let t=i.nd.filter(e=>e.slug!==n.slug&&e.tags.some(e=>n.tags.includes(e))).slice(0,3);return(0,s.jsx)("div",{className:"min-h-screen bg-black pt-24 pb-16",children:(0,s.jsx)("article",{className:"max-w-3xl mx-auto px-6",children:(0,s.jsxs)(r.E.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},children:[(0,s.jsxs)(o.default,{href:"/blog",className:"inline-flex items-center gap-2 text-white/60 hover:text-lime transition-colors mb-8",children:[(0,s.jsx)(m,{className:"w-4 h-4"}),"Back to blog"]}),(0,s.jsxs)("header",{className:"mb-12",children:[(0,s.jsx)("div",{className:"flex gap-2 mb-4",children:n.tags.map(e=>(0,s.jsx)("span",{className:"px-3 py-1 text-xs font-medium bg-lime/20 text-lime rounded-full",children:e},e))}),(0,s.jsx)("h1",{className:"text-4xl md:text-5xl font-bold text-white mb-6",children:n.title}),(0,s.jsxs)("div",{className:"flex items-center gap-4 text-white/60",children:[(0,s.jsx)("span",{children:n.author}),(0,s.jsx)("span",{children:"•"}),(0,s.jsx)("span",{children:(0,l.p)(n.date)}),(0,s.jsx)("span",{children:"•"}),(0,s.jsx)("span",{children:n.readTime})]})]}),(0,s.jsx)("div",{className:"prose prose-invert prose-lg max-w-none",children:n.content.split("\n").map((e,n)=>{if(e.startsWith("# "))return(0,s.jsx)("h1",{className:"text-3xl font-bold text-white mt-12 mb-6",children:e.slice(2)},n);if(e.startsWith("## "))return(0,s.jsx)("h2",{className:"text-2xl font-semibold text-white mt-10 mb-4",children:e.slice(3)},n);if(e.startsWith("### "))return(0,s.jsx)("h3",{className:"text-xl font-semibold text-white mt-8 mb-3",children:e.slice(4)},n);if(e.startsWith("```"))return null;if(e.startsWith("- **")){let t=e.match(/\*\*(.+?)\*\*: (.+)/);if(t)return(0,s.jsxs)("p",{className:"text-white/80 mb-2 pl-4",children:[(0,s.jsx)("strong",{className:"text-lime",children:t[1]}),": ",t[2]]},n)}if(e.startsWith("- "))return(0,s.jsxs)("p",{className:"text-white/80 mb-2 pl-4",children:["• ",e.slice(2)]},n);if(e.startsWith("|")){let t=e.split("|").filter(e=>e.trim());return t.every(e=>e.includes("---"))?null:(0,s.jsx)("div",{className:"flex border-b border-white/10 py-2",children:t.map((e,n)=>(0,s.jsx)("div",{className:"flex-1 ".concat(0===n?"font-medium text-white":"text-white/60"),children:e.trim().replace(/\*\*/g,"")},n))},n)}if(e.includes("`")){let t=e.split(/(`[^`]+`)/);return(0,s.jsx)("p",{className:"text-white/80 mb-4",children:t.map((e,n)=>e.startsWith("`")?(0,s.jsx)("code",{className:"bg-white/10 px-2 py-0.5 rounded text-lime text-sm",children:e.slice(1,-1)},n):e)},n)}return e.trim()?(0,s.jsx)("p",{className:"text-white/80 mb-4",children:e},n):null})}),t.length>0&&(0,s.jsxs)("section",{className:"mt-16 pt-12 border-t border-white/10",children:[(0,s.jsx)("h2",{className:"text-xl font-semibold text-white mb-6",children:"Related Posts"}),(0,s.jsx)("div",{className:"grid gap-4",children:t.map(e=>(0,s.jsxs)(o.default,{href:"/blog/".concat(e.slug),className:"block p-4 border border-white/10 rounded-lg hover:border-lime/50 transition-colors",children:[(0,s.jsx)("h3",{className:"text-white font-medium hover:text-lime transition-colors",children:e.title}),(0,s.jsx)("p",{className:"text-white/60 text-sm mt-1",children:e.excerpt})]},e.slug))})]})]})})})}},1604:function(e,n,t){"use strict";t.d(n,{nd:function(){return s},zl:function(){return a}});let s=[{id:"1",title:"Introducing ZSE: 3.9s Cold Starts for LLM Inference",slug:"introducing-zse",excerpt:"We're excited to announce ZSE, a new inference engine that loads 7B models in under 4 seconds with the .zse format.",content:"\n# Introducing ZSE\n\nToday we're releasing ZSE (Z Server Engine), an ultra memory-efficient LLM inference engine that achieves **3.9 second cold starts** for 7B models.\n\n## The Problem\n\nLoading large language models is slow. A typical 7B model with bitsandbytes takes 45+ seconds to load. This makes serverless deployments expensive and development iteration slow.\n\n## Our Solution\n\nZSE introduces the `.zse` format - pre-quantized model files that skip runtime quantization entirely. The result:\n\n- **7B models**: 3.9s cold start (11.6\xd7 faster)\n- **32B models**: 21.4s cold start (5.6\xd7 faster)\n- **63-72% memory savings** compared to FP16\n\n## How It Works\n\n1. **Pre-quantization**: Convert once, load fast forever\n2. **Memory mapping**: Direct tensor loading from disk\n3. **Lazy initialization**: Only load what's needed\n4. **OpenAI-compatible API**: Drop-in replacement\n\n## Try It Now\n\n```bash\npip install zllm-zse\nzse convert Qwen/Qwen2.5-7B-Instruct -o qwen-7b.zse\nzse serve qwen-7b.zse\n```\n\nWe're just getting started. Follow us for updates on zStream, zKV, and more features.\n    ",author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-25",readTime:"5 min read",tags:["announcement","performance"],featured:!0},{id:"2",title:"Complete Guide: Running Your First Model with ZSE",slug:"getting-started-tutorial",excerpt:"Step-by-step tutorial to install ZSE, convert a model, and start generating text in under 5 minutes.",content:'\n# Complete Guide: Running Your First Model with ZSE\n\nThis guide walks you through installing ZSE and running your first LLM inference in under 5 minutes.\n\n## Prerequisites\n\n- Python 3.9+\n- CUDA 11.8+ (for GPU) or CPU-only support\n- 8GB+ VRAM for 7B models\n\n## Step 1: Install ZSE\n\n```bash\npip install zllm-zse\n```\n\nVerify installation:\n```bash\nzse --version\nzse hardware  # Check GPU detection\n```\n\n## Step 2: Convert a Model\n\nConvert a HuggingFace model to .zse format:\n\n```bash\nzse convert Qwen/Qwen2.5-7B-Instruct -o qwen-7b.zse\n```\n\nThis downloads the model (~14GB), quantizes it to NF4, and saves a 4.2GB .zse file.\n\n## Step 3: Start the Server\n\n```bash\nzse serve qwen-7b.zse --port 8000\n```\n\n## Step 4: Send a Request\n\nUsing curl:\n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{"model": "qwen-7b", "messages": [{"role": "user", "content": "Hello!"}]}\'\n```\n\nOr with Python:\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="not-needed")\nresponse = client.chat.completions.create(\n    model="qwen-7b",\n    messages=[{"role": "user", "content": "Explain quantum computing"}]\n)\nprint(response.choices[0].message.content)\n```\n\n## Next Steps\n\n- Enable streaming with `stream=True`\n- Try different quantization: `--quant int4` or `--quant int8`\n- Read the [Documentation](/docs) for advanced features\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-24",readTime:"6 min read",tags:["tutorial","getting-started"],featured:!0},{id:"3",title:"Running 70B Models on a 24GB GPU with ZSE",slug:"running-70b-on-24gb",excerpt:"How to run Llama 70B and other large models on consumer GPUs using ZSE's memory optimization features.",content:"\n# Running 70B Models on a 24GB GPU\n\nYes, you can run 70B parameter models on a single RTX 4090. Here's how.\n\n## The Challenge\n\nA 70B model in FP16 needs ~140GB VRAM. Even with 4-bit quantization, that's ~35GB.\n\n## ZSE's Solution\n\nCombine multiple techniques:\n\n### 1. NF4 Quantization\n```bash\nzse convert meta-llama/Llama-3.1-70B-Instruct -o llama-70b.zse --quant nf4\n```\nBrings weights down to ~35GB.\n\n### 2. CPU Offloading\n```bash\nzse serve llama-70b.zse --offload-layers 20\n```\nKeep attention-heavy layers on GPU, offload FFN layers to CPU RAM.\n\n### 3. 4-bit KV Cache\n```bash\nzse serve llama-70b.zse --kv-quant int4 --max-context 4096\n```\nReduces KV cache memory by 4\xd7.\n\n## Full Command\n\n```bash\nzse serve llama-70b.zse \\\n  --offload-layers 20 \\\n  --kv-quant int4 \\\n  --max-context 4096 \\\n  --max-batch 4\n```\n\n## Performance Expectations\n\n| GPU | Throughput | Latency |\n|-----|------------|---------|\n| RTX 4090 24GB | ~15 tok/s | ~200ms TTFT |\n| RTX 3090 24GB | ~10 tok/s | ~300ms TTFT |\n| A100 80GB | ~45 tok/s | ~80ms TTFT |\n\n## Tips for Best Results\n\n1. **Use SSD storage** - NVMe makes offloading faster\n2. **Allocate enough RAM** - 64GB system RAM recommended\n3. **Reduce batch size** - Trade throughput for memory\n4. **Limit context length** - Shorter contexts use less KV cache\n\nNow you can run frontier models locally!\n    ",author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-23",readTime:"7 min read",tags:["tutorial","memory","advanced"],featured:!1},{id:"4",title:"ZSE Quantization Guide: NF4 vs INT4 vs INT8",slug:"quantization-guide",excerpt:"Understanding the tradeoffs between different quantization types and when to use each one.",content:"\n# ZSE Quantization Guide\n\nChoosing the right quantization is key to balancing quality, speed, and memory.\n\n## Available Quantization Types\n\n### NF4 (NormalFloat4) - Default\n```bash\nzse convert model -o model.zse --quant nf4\n```\n- **Bits**: 4\n- **Quality**: ★★★★☆ (best 4-bit)\n- **Size**: ~0.56GB per billion params\n- **Use case**: Most models, production deployments\n\nNF4 uses an asymmetric quantization grid optimized for the weight distribution of neural networks.\n\n### INT4\n```bash\nzse convert model -o model.zse --quant int4\n```\n- **Bits**: 4\n- **Quality**: ★★★☆☆\n- **Size**: ~0.53GB per billion params\n- **Use case**: Maximum compression, less sensitive tasks\n\n### INT8\n```bash\nzse convert model -o model.zse --quant int8\n```\n- **Bits**: 8\n- **Quality**: ★★★★★ (near FP16)\n- **Size**: ~1.1GB per billion params\n- **Use case**: When quality is critical\n\n### FP16 (No Quantization)\n```bash\nzse convert model -o model.zse --quant fp16\n```\n- **Bits**: 16\n- **Quality**: ★★★★★ (original)\n- **Size**: ~2GB per billion params\n- **Use case**: Fine-tuning, debugging\n\n## Quality Comparison (Qwen 7B)\n\n| Quant | Perplexity | MMLU | Size |\n|-------|------------|------|------|\n| FP16 | 5.38 | 64.8% | 14GB |\n| INT8 | 5.39 | 64.7% | 7.5GB |\n| NF4 | 5.42 | 64.2% | 4.2GB |\n| INT4 | 5.51 | 63.5% | 4.0GB |\n\n## Recommendations\n\n- **General use**: NF4 (best quality/size ratio)\n- **Code generation**: INT8 (higher precision helps)\n- **Embeddings**: INT8 or FP16\n- **Chat/creative**: NF4 is plenty\n    ",author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-22",readTime:"8 min read",tags:["technical","quantization","guide"],featured:!1},{id:"5",title:"Building a Local RAG Chatbot with ZSE",slug:"building-rag-chatbot",excerpt:"Create a retrieval-augmented generation chatbot that answers questions about your documents.",content:'\n# Building a Local RAG Chatbot with ZSE\n\nBuild a chatbot that can answer questions about your documents using ZSE\'s built-in RAG features.\n\n## What We\'re Building\n\nA chatbot that:\n1. Indexes your PDF/text documents\n2. Retrieves relevant context for questions\n3. Generates accurate answers using an LLM\n\n## Step 1: Prepare Your Model\n\n```bash\nzse convert Qwen/Qwen2.5-7B-Instruct -o qwen-7b.zse\n```\n\n## Step 2: Index Documents\n\n```python\nfrom zllm_zse import ZSE, RAGIndex\n\n# Load model\nmodel = ZSE("qwen-7b.zse")\n\n# Create index\nindex = RAGIndex(embedding_model="sentence-transformers/all-MiniLM-L6-v2")\n\n# Add documents\nindex.add_documents([\n    "docs/manual.pdf",\n    "docs/faq.txt",\n    "docs/api-reference.md"\n])\n\n# Save index\nindex.save("my_knowledge_base")\n```\n\n## Step 3: Query with Context\n\n```python\n# Load index\nindex = RAGIndex.load("my_knowledge_base")\n\n# Ask a question\nquestion = "How do I reset my password?"\ncontext = index.search(question, top_k=3)\n\n# Generate answer with context\nresponse = model.chat([\n    {"role": "system", "content": f"Answer based on this context:\\n{context}"},\n    {"role": "user", "content": question}\n])\nprint(response)\n```\n\n## Step 4: Run as API Server\n\n```bash\nzse serve qwen-7b.zse --rag-index my_knowledge_base --port 8000\n```\n\nNow your API automatically retrieves context for each query!\n\n## Tips for Better RAG\n\n1. **Chunk size matters** - Try 512-1024 tokens per chunk\n2. **Use hybrid search** - Combine semantic + keyword search\n3. **Add metadata** - Filter by document type/date\n4. **Tune retrieval** - More context isn\'t always better (3-5 chunks)\n\nYour documents stay local - nothing leaves your machine.\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-21",readTime:"10 min read",tags:["tutorial","rag","chatbot"],featured:!1},{id:"6",title:"Deploying ZSE to Production: Docker & Kubernetes",slug:"production-deployment",excerpt:"Best practices for deploying ZSE in production environments with Docker, Kubernetes, and monitoring.",content:'\n# Deploying ZSE to Production\n\nA complete guide to deploying ZSE in production with Docker, Kubernetes, and proper monitoring.\n\n## Docker Deployment\n\n### Dockerfile\n```dockerfile\nFROM nvidia/cuda:12.1-runtime-ubuntu22.04\n\nRUN pip install zllm-zse\n\nCOPY models/qwen-7b.zse /models/\nEXPOSE 8000\n\nCMD ["zse", "serve", "/models/qwen-7b.zse", "--host", "0.0.0.0"]\n```\n\n### Docker Compose\n```yaml\nversion: \'3.8\'\nservices:\n  zse:\n    build: .\n    ports:\n      - "8000:8000"\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n    volumes:\n      - ./models:/models\n    environment:\n      - ZSE_MAX_BATCH_SIZE=32\n      - ZSE_LOG_LEVEL=info\n```\n\n## Kubernetes Deployment\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zse-inference\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zse\n  template:\n    spec:\n      containers:\n      - name: zse\n        image: your-registry/zse:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: "32Gi"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          periodSeconds: 30\n```\n\n## Health Checks & Monitoring\n\n### Prometheus Metrics\nZSE exposes metrics at `/metrics`:\n- `zse_requests_total`\n- `zse_request_duration_seconds`\n- `zse_tokens_generated_total`\n- `zse_gpu_memory_bytes`\n\n### Alerting Rules\n```yaml\ngroups:\n- name: zse\n  rules:\n  - alert: HighLatency\n    expr: zse_request_duration_seconds{quantile="0.99"} > 5\n    for: 5m\n  - alert: GPUMemoryHigh\n    expr: zse_gpu_memory_bytes / zse_gpu_memory_total > 0.95\n    for: 1m\n```\n\n## Production Checklist\n\n- [ ] Set appropriate `--max-batch` and `--max-concurrent`\n- [ ] Enable request logging: `--log-format json`\n- [ ] Configure rate limiting: `--rate-limit 100`\n- [ ] Set up Prometheus scraping\n- [ ] Test graceful shutdown handling\n- [ ] Configure horizontal pod autoscaling\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-20",readTime:"12 min read",tags:["deployment","docker","kubernetes","production"],featured:!1},{id:"7",title:"Streaming Responses with ZSE: Real-time Token Generation",slug:"streaming-responses",excerpt:"Implement real-time streaming for chat applications with minimal time-to-first-token.",content:'\n# Streaming Responses with ZSE\n\nEnable real-time token streaming for responsive chat applications.\n\n## Why Streaming?\n\nWithout streaming, users wait for the entire response. With streaming:\n- **~50ms** time-to-first-token\n- Immediate visual feedback\n- Better perceived performance\n\n## Enable Streaming\n\n### REST API\n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "qwen-7b",\n    "messages": [{"role": "user", "content": "Tell me a story"}],\n    "stream": true\n  }\'\n```\n\n### Python (OpenAI SDK)\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="x")\n\nstream = client.chat.completions.create(\n    model="qwen-7b",\n    messages=[{"role": "user", "content": "Tell me a story"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end="", flush=True)\n```\n\n### JavaScript/React\n```javascript\nasync function streamChat(message) {\n  const response = await fetch(\'/v1/chat/completions\', {\n    method: \'POST\',\n    headers: { \'Content-Type\': \'application/json\' },\n    body: JSON.stringify({\n      model: \'qwen-7b\',\n      messages: [{ role: \'user\', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n    \n    const text = decoder.decode(value);\n    // Parse SSE and update UI\n    console.log(text);\n  }\n}\n```\n\n## Server-Sent Events Format\n\nEach chunk is prefixed with `data: `:\n```\ndata: {"choices":[{"delta":{"content":"Once"}}]}\ndata: {"choices":[{"delta":{"content":" upon"}}]}\ndata: {"choices":[{"delta":{"content":" a"}}]}\ndata: [DONE]\n```\n\n## Best Practices\n\n1. **Handle backpressure** - Don\'t overwhelm slow clients\n2. **Implement cancellation** - Let users stop generation\n3. **Show typing indicator** - While waiting for first token\n4. **Buffer intelligently** - Consider word-level chunks for smoother UX\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-19",readTime:"7 min read",tags:["tutorial","streaming","api"],featured:!1},{id:"8",title:"Benchmarking Your ZSE Setup: Measuring Real Performance",slug:"benchmarking-guide",excerpt:"How to accurately measure cold start time, throughput, and latency for your specific hardware.",content:'\n# Benchmarking Your ZSE Setup\n\nLearn how to measure real performance metrics for your hardware.\n\n## Built-in Benchmark Command\n\n```bash\nzse benchmark qwen-7b.zse\n```\n\nOutput:\n```\n┌────────────────────────────────────────────────┐\n│ Benchmark Results: qwen-7b.zse                 │\n├────────────────────────────────────────────────┤\n│ Cold Start:         3.9s                       │\n│ Throughput:         87.3 tok/s                 │\n│ Time-to-First:      52ms                       │\n│ Latency (p50):      11.4ms/tok                 │\n│ Latency (p99):      18.2ms/tok                 │\n│ GPU Memory:         5.2 GB                     │\n└────────────────────────────────────────────────┘\n```\n\n## Specific Benchmarks\n\n### Cold Start Only\n```bash\nzse benchmark model.zse --metric cold-start --runs 5\n```\n\n### Throughput Test\n```bash\nzse benchmark model.zse --metric throughput \\\n  --prompt-length 512 \\\n  --output-length 256 \\\n  --batch-sizes 1,4,8,16\n```\n\n### Memory Profiling\n```bash\nzse benchmark model.zse --metric memory \\\n  --context-lengths 1024,4096,8192,16384\n```\n\n## Compare Configurations\n\n```bash\n# Compare quantization types\nzse benchmark model-nf4.zse model-int4.zse model-int8.zse\n\n# Compare context lengths\nzse benchmark model.zse --sweep max-context 1024:16384:2x\n```\n\n## Python Benchmarking\n\n```python\nfrom zllm_zse import ZSE, benchmark\n\nmodel = ZSE("qwen-7b.zse")\n\nresults = benchmark(\n    model,\n    prompts=["Explain quantum computing" * 10 for _ in range(100)],\n    max_tokens=256\n)\n\nprint(f"Mean throughput: {results.throughput_mean:.1f} tok/s")\nprint(f"p99 latency: {results.latency_p99:.1f} ms/tok")\n```\n\n## Hardware-Specific Expectations\n\n| GPU | 7B Throughput | 7B Cold Start |\n|-----|---------------|---------------|\n| RTX 3060 12GB | ~45 tok/s | ~4.5s |\n| RTX 4070 12GB | ~80 tok/s | ~4.0s |\n| RTX 4090 24GB | ~120 tok/s | ~3.5s |\n| A100 80GB | ~180 tok/s | ~3.2s |\n\nYour mileage may vary based on PCIe bandwidth, CPU, and storage speed.\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-18",readTime:"6 min read",tags:["benchmarks","performance","guide"],featured:!1}];function a(e){return s.find(n=>n.slug===e)}},2169:function(e,n,t){"use strict";t.d(n,{cn:function(){return o},p:function(){return r}});var s=t(3167),a=t(1367);function o(){for(var e=arguments.length,n=Array(e),t=0;t<e;t++)n[t]=arguments[t];return(0,a.m6)((0,s.W)(n))}function r(e){return new Date(e).toLocaleDateString("en-US",{year:"numeric",month:"long",day:"numeric"})}}},function(e){e.O(0,[5293,2150,8792,2971,8069,1744],function(){return e(e.s=2814)}),_N_E=e.O()}]);