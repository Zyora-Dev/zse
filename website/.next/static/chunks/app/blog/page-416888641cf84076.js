(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[9404],{6009:function(n,e,t){Promise.resolve().then(t.bind(t,4497))},4497:function(n,e,t){"use strict";t.r(e),t.d(e,{default:function(){return m}});var a=t(3827),s=t(8792),o=t(5293),r=t(1604),i=t(2169);function l(n){let{post:e,index:t}=n;return(0,a.jsx)(o.E.article,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},transition:{duration:.5,delay:.1*t},className:"group",children:(0,a.jsx)(s.default,{href:"/blog/".concat(e.slug),children:(0,a.jsxs)("div",{className:"border border-white/10 rounded-xl p-6 hover:border-lime/50 transition-all duration-300 bg-white/[0.02] hover:bg-white/[0.04]",children:[e.featured&&(0,a.jsx)("span",{className:"inline-block px-3 py-1 text-xs font-medium bg-lime/20 text-lime rounded-full mb-4",children:"Featured"}),(0,a.jsx)("h2",{className:"text-xl font-semibold text-white mb-3 group-hover:text-lime transition-colors",children:e.title}),(0,a.jsx)("p",{className:"text-white/60 mb-4 line-clamp-2",children:e.excerpt}),(0,a.jsxs)("div",{className:"flex items-center justify-between text-sm text-white/40",children:[(0,a.jsxs)("div",{className:"flex items-center gap-2",children:[(0,a.jsx)("span",{children:e.author}),(0,a.jsx)("span",{children:"•"}),(0,a.jsx)("span",{children:(0,i.p)(e.date)})]}),(0,a.jsx)("span",{children:e.readTime})]}),(0,a.jsx)("div",{className:"flex gap-2 mt-4",children:e.tags.map(n=>(0,a.jsx)("span",{className:"px-2 py-1 text-xs bg-white/5 text-white/50 rounded",children:n},n))})]})})})}function m(){let n=r.nd.filter(n=>n.featured),e=r.nd.filter(n=>!n.featured);return(0,a.jsx)("div",{className:"min-h-screen bg-black pt-24 pb-16",children:(0,a.jsxs)("div",{className:"max-w-5xl mx-auto px-6",children:[(0,a.jsxs)(o.E.div,{initial:{opacity:0,y:20},animate:{opacity:1,y:0},className:"mb-12",children:[(0,a.jsx)("h1",{className:"text-4xl md:text-5xl font-bold text-white mb-4",children:"Blog"}),(0,a.jsx)("p",{className:"text-xl text-white/60",children:"Updates, tutorials, and insights from the ZSE team"})]}),n.length>0&&(0,a.jsxs)("section",{className:"mb-12",children:[(0,a.jsx)("h2",{className:"text-lg font-semibold text-lime mb-6",children:"Featured"}),(0,a.jsx)("div",{className:"space-y-6",children:n.map((n,e)=>(0,a.jsx)(l,{post:n,index:e},n.id))})]}),(0,a.jsxs)("section",{children:[(0,a.jsx)("h2",{className:"text-lg font-semibold text-white/80 mb-6",children:"All Posts"}),(0,a.jsx)("div",{className:"space-y-6",children:e.map((e,t)=>(0,a.jsx)(l,{post:e,index:t+n.length},e.id))})]}),0===r.nd.length&&(0,a.jsx)("div",{className:"text-center py-12",children:(0,a.jsx)("p",{className:"text-white/60",children:"No blog posts yet. Check back soon!"})})]})})}},1604:function(n,e,t){"use strict";t.d(e,{nd:function(){return a},zl:function(){return s}});let a=[{id:"1",title:"Introducing ZSE: 3.9s Cold Starts for LLM Inference",slug:"introducing-zse",excerpt:"We're excited to announce ZSE, a new inference engine that loads 7B models in under 4 seconds with the .zse format.",content:"\n# Introducing ZSE\n\nToday we're releasing ZSE (Z Server Engine), an ultra memory-efficient LLM inference engine that achieves **3.9 second cold starts** for 7B models.\n\n## The Problem\n\nLoading large language models is slow. A typical 7B model with bitsandbytes takes 45+ seconds to load. This makes serverless deployments expensive and development iteration slow.\n\n## Our Solution\n\nZSE introduces the `.zse` format - pre-quantized model files that skip runtime quantization entirely. The result:\n\n- **7B models**: 3.9s cold start (11.6\xd7 faster)\n- **32B models**: 21.4s cold start (5.6\xd7 faster)\n- **63-72% memory savings** compared to FP16\n\n## How It Works\n\n1. **Pre-quantization**: Convert once, load fast forever\n2. **Memory mapping**: Direct tensor loading from disk\n3. **Lazy initialization**: Only load what's needed\n4. **OpenAI-compatible API**: Drop-in replacement\n\n## Try It Now\n\n```bash\npip install zllm-zse\nzse convert Qwen/Qwen2.5-7B-Instruct -o qwen-7b.zse\nzse serve qwen-7b.zse\n```\n\nWe're just getting started. Follow us for updates on zStream, zKV, and more features.\n    ",author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-25",readTime:"5 min read",tags:["announcement","performance"],featured:!0},{id:"2",title:"Complete Guide: Running Your First Model with ZSE",slug:"getting-started-tutorial",excerpt:"Step-by-step tutorial to install ZSE, convert a model, and start generating text in under 5 minutes.",content:'\n# Complete Guide: Running Your First Model with ZSE\n\nThis guide walks you through installing ZSE and running your first LLM inference in under 5 minutes.\n\n## Prerequisites\n\n- Python 3.9+\n- CUDA 11.8+ (for GPU) or CPU-only support\n- 8GB+ VRAM for 7B models\n\n## Step 1: Install ZSE\n\n```bash\npip install zllm-zse\n```\n\nVerify installation:\n```bash\nzse --version\nzse hardware  # Check GPU detection\n```\n\n## Step 2: Convert a Model\n\nConvert a HuggingFace model to .zse format:\n\n```bash\nzse convert Qwen/Qwen2.5-7B-Instruct -o qwen-7b.zse\n```\n\nThis downloads the model (~14GB), quantizes it to NF4, and saves a 4.2GB .zse file.\n\n## Step 3: Start the Server\n\n```bash\nzse serve qwen-7b.zse --port 8000\n```\n\n## Step 4: Send a Request\n\nUsing curl:\n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{"model": "qwen-7b", "messages": [{"role": "user", "content": "Hello!"}]}\'\n```\n\nOr with Python:\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="not-needed")\nresponse = client.chat.completions.create(\n    model="qwen-7b",\n    messages=[{"role": "user", "content": "Explain quantum computing"}]\n)\nprint(response.choices[0].message.content)\n```\n\n## Next Steps\n\n- Enable streaming with `stream=True`\n- Try different quantization: `--quant int4` or `--quant int8`\n- Read the [Documentation](/docs) for advanced features\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-24",readTime:"6 min read",tags:["tutorial","getting-started"],featured:!0},{id:"3",title:"Running 70B Models on a 24GB GPU with ZSE",slug:"running-70b-on-24gb",excerpt:"How to run Llama 70B and other large models on consumer GPUs using ZSE's memory optimization features.",content:"\n# Running 70B Models on a 24GB GPU\n\nYes, you can run 70B parameter models on a single RTX 4090. Here's how.\n\n## The Challenge\n\nA 70B model in FP16 needs ~140GB VRAM. Even with 4-bit quantization, that's ~35GB.\n\n## ZSE's Solution\n\nCombine multiple techniques:\n\n### 1. NF4 Quantization\n```bash\nzse convert meta-llama/Llama-3.1-70B-Instruct -o llama-70b.zse --quant nf4\n```\nBrings weights down to ~35GB.\n\n### 2. CPU Offloading\n```bash\nzse serve llama-70b.zse --offload-layers 20\n```\nKeep attention-heavy layers on GPU, offload FFN layers to CPU RAM.\n\n### 3. 4-bit KV Cache\n```bash\nzse serve llama-70b.zse --kv-quant int4 --max-context 4096\n```\nReduces KV cache memory by 4\xd7.\n\n## Full Command\n\n```bash\nzse serve llama-70b.zse \\\n  --offload-layers 20 \\\n  --kv-quant int4 \\\n  --max-context 4096 \\\n  --max-batch 4\n```\n\n## Performance Expectations\n\n| GPU | Throughput | Latency |\n|-----|------------|---------|\n| RTX 4090 24GB | ~15 tok/s | ~200ms TTFT |\n| RTX 3090 24GB | ~10 tok/s | ~300ms TTFT |\n| A100 80GB | ~45 tok/s | ~80ms TTFT |\n\n## Tips for Best Results\n\n1. **Use SSD storage** - NVMe makes offloading faster\n2. **Allocate enough RAM** - 64GB system RAM recommended\n3. **Reduce batch size** - Trade throughput for memory\n4. **Limit context length** - Shorter contexts use less KV cache\n\nNow you can run frontier models locally!\n    ",author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-23",readTime:"7 min read",tags:["tutorial","memory","advanced"],featured:!1},{id:"4",title:"ZSE Quantization Guide: NF4 vs INT4 vs INT8",slug:"quantization-guide",excerpt:"Understanding the tradeoffs between different quantization types and when to use each one.",content:"\n# ZSE Quantization Guide\n\nChoosing the right quantization is key to balancing quality, speed, and memory.\n\n## Available Quantization Types\n\n### NF4 (NormalFloat4) - Default\n```bash\nzse convert model -o model.zse --quant nf4\n```\n- **Bits**: 4\n- **Quality**: ★★★★☆ (best 4-bit)\n- **Size**: ~0.56GB per billion params\n- **Use case**: Most models, production deployments\n\nNF4 uses an asymmetric quantization grid optimized for the weight distribution of neural networks.\n\n### INT4\n```bash\nzse convert model -o model.zse --quant int4\n```\n- **Bits**: 4\n- **Quality**: ★★★☆☆\n- **Size**: ~0.53GB per billion params\n- **Use case**: Maximum compression, less sensitive tasks\n\n### INT8\n```bash\nzse convert model -o model.zse --quant int8\n```\n- **Bits**: 8\n- **Quality**: ★★★★★ (near FP16)\n- **Size**: ~1.1GB per billion params\n- **Use case**: When quality is critical\n\n### FP16 (No Quantization)\n```bash\nzse convert model -o model.zse --quant fp16\n```\n- **Bits**: 16\n- **Quality**: ★★★★★ (original)\n- **Size**: ~2GB per billion params\n- **Use case**: Fine-tuning, debugging\n\n## Quality Comparison (Qwen 7B)\n\n| Quant | Perplexity | MMLU | Size |\n|-------|------------|------|------|\n| FP16 | 5.38 | 64.8% | 14GB |\n| INT8 | 5.39 | 64.7% | 7.5GB |\n| NF4 | 5.42 | 64.2% | 4.2GB |\n| INT4 | 5.51 | 63.5% | 4.0GB |\n\n## Recommendations\n\n- **General use**: NF4 (best quality/size ratio)\n- **Code generation**: INT8 (higher precision helps)\n- **Embeddings**: INT8 or FP16\n- **Chat/creative**: NF4 is plenty\n    ",author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-22",readTime:"8 min read",tags:["technical","quantization","guide"],featured:!1},{id:"5",title:"Building a Local RAG Chatbot with ZSE",slug:"building-rag-chatbot",excerpt:"Create a retrieval-augmented generation chatbot that answers questions about your documents.",content:'\n# Building a Local RAG Chatbot with ZSE\n\nBuild a chatbot that can answer questions about your documents using ZSE\'s built-in RAG features.\n\n## What We\'re Building\n\nA chatbot that:\n1. Indexes your PDF/text documents\n2. Retrieves relevant context for questions\n3. Generates accurate answers using an LLM\n\n## Step 1: Prepare Your Model\n\n```bash\nzse convert Qwen/Qwen2.5-7B-Instruct -o qwen-7b.zse\n```\n\n## Step 2: Index Documents\n\n```python\nfrom zllm_zse import ZSE, RAGIndex\n\n# Load model\nmodel = ZSE("qwen-7b.zse")\n\n# Create index\nindex = RAGIndex(embedding_model="sentence-transformers/all-MiniLM-L6-v2")\n\n# Add documents\nindex.add_documents([\n    "docs/manual.pdf",\n    "docs/faq.txt",\n    "docs/api-reference.md"\n])\n\n# Save index\nindex.save("my_knowledge_base")\n```\n\n## Step 3: Query with Context\n\n```python\n# Load index\nindex = RAGIndex.load("my_knowledge_base")\n\n# Ask a question\nquestion = "How do I reset my password?"\ncontext = index.search(question, top_k=3)\n\n# Generate answer with context\nresponse = model.chat([\n    {"role": "system", "content": f"Answer based on this context:\\n{context}"},\n    {"role": "user", "content": question}\n])\nprint(response)\n```\n\n## Step 4: Run as API Server\n\n```bash\nzse serve qwen-7b.zse --rag-index my_knowledge_base --port 8000\n```\n\nNow your API automatically retrieves context for each query!\n\n## Tips for Better RAG\n\n1. **Chunk size matters** - Try 512-1024 tokens per chunk\n2. **Use hybrid search** - Combine semantic + keyword search\n3. **Add metadata** - Filter by document type/date\n4. **Tune retrieval** - More context isn\'t always better (3-5 chunks)\n\nYour documents stay local - nothing leaves your machine.\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-21",readTime:"10 min read",tags:["tutorial","rag","chatbot"],featured:!1},{id:"6",title:"Deploying ZSE to Production: Docker & Kubernetes",slug:"production-deployment",excerpt:"Best practices for deploying ZSE in production environments with Docker, Kubernetes, and monitoring.",content:'\n# Deploying ZSE to Production\n\nA complete guide to deploying ZSE in production with Docker, Kubernetes, and proper monitoring.\n\n## Docker Deployment\n\n### Dockerfile\n```dockerfile\nFROM nvidia/cuda:12.1-runtime-ubuntu22.04\n\nRUN pip install zllm-zse\n\nCOPY models/qwen-7b.zse /models/\nEXPOSE 8000\n\nCMD ["zse", "serve", "/models/qwen-7b.zse", "--host", "0.0.0.0"]\n```\n\n### Docker Compose\n```yaml\nversion: \'3.8\'\nservices:\n  zse:\n    build: .\n    ports:\n      - "8000:8000"\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n    volumes:\n      - ./models:/models\n    environment:\n      - ZSE_MAX_BATCH_SIZE=32\n      - ZSE_LOG_LEVEL=info\n```\n\n## Kubernetes Deployment\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zse-inference\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zse\n  template:\n    spec:\n      containers:\n      - name: zse\n        image: your-registry/zse:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n            memory: "32Gi"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          periodSeconds: 30\n```\n\n## Health Checks & Monitoring\n\n### Prometheus Metrics\nZSE exposes metrics at `/metrics`:\n- `zse_requests_total`\n- `zse_request_duration_seconds`\n- `zse_tokens_generated_total`\n- `zse_gpu_memory_bytes`\n\n### Alerting Rules\n```yaml\ngroups:\n- name: zse\n  rules:\n  - alert: HighLatency\n    expr: zse_request_duration_seconds{quantile="0.99"} > 5\n    for: 5m\n  - alert: GPUMemoryHigh\n    expr: zse_gpu_memory_bytes / zse_gpu_memory_total > 0.95\n    for: 1m\n```\n\n## Production Checklist\n\n- [ ] Set appropriate `--max-batch` and `--max-concurrent`\n- [ ] Enable request logging: `--log-format json`\n- [ ] Configure rate limiting: `--rate-limit 100`\n- [ ] Set up Prometheus scraping\n- [ ] Test graceful shutdown handling\n- [ ] Configure horizontal pod autoscaling\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-20",readTime:"12 min read",tags:["deployment","docker","kubernetes","production"],featured:!1},{id:"7",title:"Streaming Responses with ZSE: Real-time Token Generation",slug:"streaming-responses",excerpt:"Implement real-time streaming for chat applications with minimal time-to-first-token.",content:'\n# Streaming Responses with ZSE\n\nEnable real-time token streaming for responsive chat applications.\n\n## Why Streaming?\n\nWithout streaming, users wait for the entire response. With streaming:\n- **~50ms** time-to-first-token\n- Immediate visual feedback\n- Better perceived performance\n\n## Enable Streaming\n\n### REST API\n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "qwen-7b",\n    "messages": [{"role": "user", "content": "Tell me a story"}],\n    "stream": true\n  }\'\n```\n\n### Python (OpenAI SDK)\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="x")\n\nstream = client.chat.completions.create(\n    model="qwen-7b",\n    messages=[{"role": "user", "content": "Tell me a story"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end="", flush=True)\n```\n\n### JavaScript/React\n```javascript\nasync function streamChat(message) {\n  const response = await fetch(\'/v1/chat/completions\', {\n    method: \'POST\',\n    headers: { \'Content-Type\': \'application/json\' },\n    body: JSON.stringify({\n      model: \'qwen-7b\',\n      messages: [{ role: \'user\', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n    \n    const text = decoder.decode(value);\n    // Parse SSE and update UI\n    console.log(text);\n  }\n}\n```\n\n## Server-Sent Events Format\n\nEach chunk is prefixed with `data: `:\n```\ndata: {"choices":[{"delta":{"content":"Once"}}]}\ndata: {"choices":[{"delta":{"content":" upon"}}]}\ndata: {"choices":[{"delta":{"content":" a"}}]}\ndata: [DONE]\n```\n\n## Best Practices\n\n1. **Handle backpressure** - Don\'t overwhelm slow clients\n2. **Implement cancellation** - Let users stop generation\n3. **Show typing indicator** - While waiting for first token\n4. **Buffer intelligently** - Consider word-level chunks for smoother UX\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-19",readTime:"7 min read",tags:["tutorial","streaming","api"],featured:!1},{id:"8",title:"Benchmarking Your ZSE Setup: Measuring Real Performance",slug:"benchmarking-guide",excerpt:"How to accurately measure cold start time, throughput, and latency for your specific hardware.",content:'\n# Benchmarking Your ZSE Setup\n\nLearn how to measure real performance metrics for your hardware.\n\n## Built-in Benchmark Command\n\n```bash\nzse benchmark qwen-7b.zse\n```\n\nOutput:\n```\n┌────────────────────────────────────────────────┐\n│ Benchmark Results: qwen-7b.zse                 │\n├────────────────────────────────────────────────┤\n│ Cold Start:         3.9s                       │\n│ Throughput:         87.3 tok/s                 │\n│ Time-to-First:      52ms                       │\n│ Latency (p50):      11.4ms/tok                 │\n│ Latency (p99):      18.2ms/tok                 │\n│ GPU Memory:         5.2 GB                     │\n└────────────────────────────────────────────────┘\n```\n\n## Specific Benchmarks\n\n### Cold Start Only\n```bash\nzse benchmark model.zse --metric cold-start --runs 5\n```\n\n### Throughput Test\n```bash\nzse benchmark model.zse --metric throughput \\\n  --prompt-length 512 \\\n  --output-length 256 \\\n  --batch-sizes 1,4,8,16\n```\n\n### Memory Profiling\n```bash\nzse benchmark model.zse --metric memory \\\n  --context-lengths 1024,4096,8192,16384\n```\n\n## Compare Configurations\n\n```bash\n# Compare quantization types\nzse benchmark model-nf4.zse model-int4.zse model-int8.zse\n\n# Compare context lengths\nzse benchmark model.zse --sweep max-context 1024:16384:2x\n```\n\n## Python Benchmarking\n\n```python\nfrom zllm_zse import ZSE, benchmark\n\nmodel = ZSE("qwen-7b.zse")\n\nresults = benchmark(\n    model,\n    prompts=["Explain quantum computing" * 10 for _ in range(100)],\n    max_tokens=256\n)\n\nprint(f"Mean throughput: {results.throughput_mean:.1f} tok/s")\nprint(f"p99 latency: {results.latency_p99:.1f} ms/tok")\n```\n\n## Hardware-Specific Expectations\n\n| GPU | 7B Throughput | 7B Cold Start |\n|-----|---------------|---------------|\n| RTX 3060 12GB | ~45 tok/s | ~4.5s |\n| RTX 4070 12GB | ~80 tok/s | ~4.0s |\n| RTX 4090 24GB | ~120 tok/s | ~3.5s |\n| A100 80GB | ~180 tok/s | ~3.2s |\n\nYour mileage may vary based on PCIe bandwidth, CPU, and storage speed.\n    ',author:"ZSE Team",authorImage:"/images/zllm-logo.png",date:"2026-02-18",readTime:"6 min read",tags:["benchmarks","performance","guide"],featured:!1}];function s(n){return a.find(e=>e.slug===n)}},2169:function(n,e,t){"use strict";t.d(e,{cn:function(){return o},p:function(){return r}});var a=t(3167),s=t(1367);function o(){for(var n=arguments.length,e=Array(n),t=0;t<n;t++)e[t]=arguments[t];return(0,s.m6)((0,a.W)(e))}function r(n){return new Date(n).toLocaleDateString("en-US",{year:"numeric",month:"long",day:"numeric"})}}},function(n){n.O(0,[5293,2150,8792,2971,8069,1744],function(){return n(n.s=6009)}),_N_E=n.O()}]);