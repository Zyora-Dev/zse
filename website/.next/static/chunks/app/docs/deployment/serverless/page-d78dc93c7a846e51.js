(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[9360],{1893:function(e,t,n){Promise.resolve().then(n.bind(n,4586))},4586:function(e,t,n){"use strict";n.r(t),n.d(t,{default:function(){return d}});var s=n(3827),r=n(9676),l=n(5558),o=n(4657),a=n(4987);let i=[{id:"overview",title:"Overview",level:2},{id:"modal",title:"Modal",level:2},{id:"runpod",title:"RunPod",level:2},{id:"replicate",title:"Replicate",level:2},{id:"aws-lambda",title:"AWS Lambda",level:2},{id:"cold-start",title:"Cold Start Optimization",level:2}];function d(){return(0,s.jsxs)("div",{className:"flex",children:[(0,s.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[(0,s.jsx)(r.lv,{title:"Serverless Deployment",description:"Deploy ZSE on serverless GPU platforms for pay-per-use inference with automatic scaling.",badge:"Deployment"}),(0,s.jsxs)(r.Je,{id:"overview",title:"Overview",children:[(0,s.jsx)("p",{className:"mb-4",children:"Serverless GPU platforms let you run ZSE without managing infrastructure. Pay only for compute time used, with automatic scaling from zero to thousands of concurrent requests."}),(0,s.jsxs)(a.gy,{columns:2,children:[(0,s.jsx)(a.Zb,{title:"Pay-per-use",description:"No idle GPU costs, pay only for inference time"}),(0,s.jsx)(a.Zb,{title:"Auto-scaling",description:"Scale to zero or thousands automatically"})]}),(0,s.jsx)(o.U,{type:"info",children:"ZSE's fast cold start (3.9s for 7B models) makes it ideal for serverless where containers may be created on-demand."})]}),(0,s.jsxs)(r.Je,{id:"modal",title:"Modal",children:[(0,s.jsx)("p",{className:"mb-4",children:"Modal is recommended for ZSE serverless deployments due to its excellent GPU support and fast container start times."}),(0,s.jsx)(r.KU,{id:"modal-setup",title:"Setup",children:(0,s.jsx)(l.d,{language:"bash",code:"# Install Modal CLI\npip install modal\n\n# Authenticate\nmodal token new"})}),(0,s.jsx)(r.KU,{id:"modal-app",title:"Modal Application",children:(0,s.jsx)(l.d,{language:"python",filename:"app.py",code:'import modal\n\napp = modal.App("zse-inference")\n\n# Define the image with ZSE\nimage = modal.Image.debian_slim().pip_install("zllm-zse")\n\n# Create a volume for models\nvolume = modal.Volume.from_name("zse-models", create_if_missing=True)\n\n@app.cls(\n    image=image,\n    gpu="A10G",  # or "A100", "T4", "H100"\n    volumes={"/models": volume},\n    container_idle_timeout=300,  # Keep warm for 5 mins\n)\nclass ZSEServer:\n    @modal.enter()\n    def load_model(self):\n        from zse.engine.orchestrator import IntelligenceOrchestrator\n        self.orch = IntelligenceOrchestrator("/models/qwen7b.zse")\n        self.orch.load()\n\n    @modal.method()\n    def generate(self, prompt: str, max_tokens: int = 256):\n        return self.orch.generate(prompt, max_tokens=max_tokens)\n\n    @modal.method()\n    def chat(self, messages: list):\n        return self.orch.chat(messages)\n\n# Web endpoint\n@app.function(image=image, gpu="A10G", volumes={"/models": volume})\n@modal.web_endpoint(method="POST")\ndef chat_endpoint(request: dict):\n    from zse.engine.orchestrator import IntelligenceOrchestrator\n    orch = IntelligenceOrchestrator("/models/qwen7b.zse")\n    orch.load()\n    return {"response": orch.chat(request["messages"])}'})}),(0,s.jsx)(r.KU,{id:"modal-deploy",title:"Deploy",children:(0,s.jsx)(l.d,{language:"bash",code:'# Upload model to volume first\nmodal volume put zse-models ./qwen7b.zse /qwen7b.zse\n\n# Deploy the app\nmodal deploy app.py\n\n# Test the endpoint\ncurl -X POST https://your-app--chat-endpoint.modal.run \\\n  -H "Content-Type: application/json" \\\n  -d \'{"messages": [{"role": "user", "content": "Hello!"}]}\''})})]}),(0,s.jsxs)(r.Je,{id:"runpod",title:"RunPod",children:[(0,s.jsx)("p",{className:"mb-4",children:"RunPod offers serverless GPU endpoints with competitive pricing."}),(0,s.jsx)(r.KU,{id:"runpod-handler",title:"Handler",children:(0,s.jsx)(l.d,{language:"python",filename:"handler.py",code:'import runpod\nfrom zse.engine.orchestrator import IntelligenceOrchestrator\n\n# Load model globally (persists across requests)\norch = None\n\ndef load_model():\n    global orch\n    if orch is None:\n        orch = IntelligenceOrchestrator("/models/qwen7b.zse")\n        orch.load()\n    return orch\n\ndef handler(job):\n    """Handle inference requests."""\n    job_input = job["input"]\n    \n    orch = load_model()\n    \n    if "messages" in job_input:\n        # Chat completion\n        response = orch.chat(job_input["messages"])\n    else:\n        # Text generation\n        response = orch.generate(\n            job_input.get("prompt", ""),\n            max_tokens=job_input.get("max_tokens", 256)\n        )\n    \n    return {"response": response}\n\nrunpod.serverless.start({"handler": handler})'})}),(0,s.jsx)(r.KU,{id:"runpod-dockerfile",title:"Dockerfile",children:(0,s.jsx)(l.d,{language:"dockerfile",filename:"Dockerfile",code:'FROM runpod/pytorch:2.1.0-py3.10-cuda12.1.0-devel-ubuntu22.04\n\n# Install ZSE\nRUN pip install zllm-zse\n\n# Copy model\nCOPY ./qwen7b.zse /models/\n\n# Copy handler\nCOPY handler.py /handler.py\n\nCMD ["python", "-u", "/handler.py"]'})})]}),(0,s.jsxs)(r.Je,{id:"replicate",title:"Replicate",children:[(0,s.jsx)("p",{className:"mb-4",children:"Deploy ZSE models on Replicate with Cog for easy API creation."}),(0,s.jsx)(r.KU,{id:"replicate-cog",title:"Cog Configuration",children:(0,s.jsx)(l.d,{language:"yaml",filename:"cog.yaml",code:'build:\n  gpu: true\n  python_version: "3.10"\n  python_packages:\n    - "zllm-zse"\n\npredict: "predict.py:Predictor"'})}),(0,s.jsx)(r.KU,{id:"replicate-predict",title:"Predictor",children:(0,s.jsx)(l.d,{language:"python",filename:"predict.py",code:'from cog import BasePredictor, Input\nfrom zse.engine.orchestrator import IntelligenceOrchestrator\n\nclass Predictor(BasePredictor):\n    def setup(self):\n        """Load the model."""\n        self.orch = IntelligenceOrchestrator("./qwen7b.zse")\n        self.orch.load()\n\n    def predict(\n        self,\n        prompt: str = Input(description="Input prompt"),\n        max_tokens: int = Input(description="Max tokens", default=256),\n        temperature: float = Input(description="Temperature", default=0.7),\n    ) -> str:\n        """Run inference."""\n        return self.orch.generate(\n            prompt,\n            max_tokens=max_tokens,\n            temperature=temperature\n        )'})}),(0,s.jsx)(r.KU,{id:"replicate-deploy",title:"Deploy",children:(0,s.jsx)(l.d,{language:"bash",code:"# Login to Replicate\ncog login\n\n# Push to Replicate\ncog push r8.im/username/zse-qwen7b"})})]}),(0,s.jsxs)(r.Je,{id:"aws-lambda",title:"AWS Lambda",children:[(0,s.jsx)("p",{className:"mb-4",children:"For AWS deployments, use Lambda with container images and Provisioned Concurrency."}),(0,s.jsx)(o.U,{type:"warning",children:"AWS Lambda has a 10GB container size limit and 15-minute timeout. For larger models, use SageMaker or EC2 with auto-scaling instead."}),(0,s.jsx)(r.KU,{id:"lambda-container",title:"Container Setup",children:(0,s.jsx)(l.d,{language:"dockerfile",filename:"Dockerfile",code:'FROM public.ecr.aws/lambda/python:3.10\n\n# Install ZSE (CPU-only for Lambda)\nRUN pip install zllm-zse\n\n# Copy model (must be < 10GB total)\nCOPY ./tinyllama.zse /models/\n\n# Copy handler\nCOPY app.py ${LAMBDA_TASK_ROOT}\n\nCMD ["app.handler"]'})}),(0,s.jsx)(r.KU,{id:"lambda-handler",title:"Handler",children:(0,s.jsx)(l.d,{language:"python",filename:"app.py",code:'from zse.engine.orchestrator import IntelligenceOrchestrator\n\n# Load model outside handler for warm starts\norch = IntelligenceOrchestrator("/models/tinyllama.zse")\norch.load()\n\ndef handler(event, context):\n    prompt = event.get("prompt", "")\n    max_tokens = event.get("max_tokens", 128)\n    \n    response = orch.generate(prompt, max_tokens=max_tokens)\n    \n    return {\n        "statusCode": 200,\n        "body": {"response": response}\n    }'})})]}),(0,s.jsxs)(r.Je,{id:"cold-start",title:"Cold Start Optimization",children:[(0,s.jsx)("p",{className:"mb-4",children:"Cold starts are critical for serverless. ZSE's .zse format provides significantly faster cold starts than alternatives."}),(0,s.jsx)("div",{className:"overflow-x-auto",children:(0,s.jsxs)("table",{className:"w-full text-sm",children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{className:"border-b border-white/10",children:[(0,s.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Format"}),(0,s.jsx)("th",{className:"text-right py-3 px-4 font-medium text-gray-400",children:"7B Cold Start"}),(0,s.jsx)("th",{className:"text-right py-3 px-4 font-medium text-gray-400",children:"Serverless Cost Impact"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{className:"border-b border-white/5 bg-lime/5",children:[(0,s.jsx)("td",{className:"py-3 px-4 text-lime font-medium",children:".zse format"}),(0,s.jsx)("td",{className:"py-3 px-4 text-right text-lime font-mono",children:"3.9s"}),(0,s.jsx)("td",{className:"py-3 px-4 text-right text-lime",children:"Minimal startup overhead"})]}),(0,s.jsxs)("tr",{className:"border-b border-white/5",children:[(0,s.jsx)("td",{className:"py-3 px-4 text-white",children:"bitsandbytes"}),(0,s.jsx)("td",{className:"py-3 px-4 text-right text-gray-400",children:"45.4s"}),(0,s.jsx)("td",{className:"py-3 px-4 text-right text-gray-400",children:"~40s billed startup"})]})]})]})}),(0,s.jsx)(a.VS,{features:["Always use pre-converted .zse models","Use container idle timeout to keep warm","Consider provisioned concurrency for consistent latency","Store models in fast storage (NVMe volumes)"]}),(0,s.jsx)(l.d,{language:"bash",code:"# Convert model before deployment (one-time)\nzse convert Qwen/Qwen2.5-7B-Instruct -o qwen7b.zse\n\n# Include .zse file in container (NOT HuggingFace weights)"})]}),(0,s.jsx)(r.KO,{prev:{href:"/docs/deployment/kubernetes",title:"Kubernetes"},next:{href:"/docs/advanced/custom-models",title:"Custom Models"}})]}),(0,s.jsx)(r.o5,{items:i})]})}},4987:function(e,t,n){"use strict";n.d(t,{Rg:function(){return a},VS:function(){return i},Zb:function(){return d},gy:function(){return c}});var s=n(3827),r=n(5293),l=n(9259),o=n(2169);function a(e){let{steps:t}=e;return(0,s.jsx)("div",{className:"my-6 space-y-0",children:t.map((e,n)=>(0,s.jsxs)(r.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*n},className:"relative pl-8 pb-8 last:pb-0",children:[n<t.length-1&&(0,s.jsx)("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),(0,s.jsx)("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:(0,s.jsx)("span",{className:"text-xs font-bold text-lime",children:n+1})}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:e.title}),e.description&&(0,s.jsx)("p",{className:"text-sm text-white/50 mb-3",children:e.description}),e.code&&(0,s.jsx)("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:(0,s.jsx)("code",{className:"text-sm text-lime/90 font-mono",children:e.code})}),e.content&&(0,s.jsx)("div",{className:"text-sm text-white/70",children:e.content})]})]},n))})}function i(e){let{features:t}=e;return(0,s.jsx)("ul",{className:"my-4 space-y-2",children:t.map((e,t)=>(0,s.jsxs)(r.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*t},className:"flex items-start gap-2",children:[(0,s.jsx)(l.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),(0,s.jsx)("span",{className:"text-sm text-white/70",children:e})]},t))})}function d(e){let{title:t,description:n,icon:l,href:a,children:i}=e,d=a?"a":"div";return(0,s.jsx)(r.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:a?{y:-2}:void 0,children:(0,s.jsxs)(d,{...a?{href:a,className:"block"}:{},className:(0,o.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",a&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[l&&(0,s.jsx)("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:(0,s.jsx)(l,{className:"w-4 h-4 text-lime"})}),(0,s.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:t}),n&&(0,s.jsx)("p",{className:"text-sm text-white/50",children:n}),i]})})}function c(e){let{children:t,columns:n=2}=e;return(0,s.jsx)("div",{className:(0,o.cn)("grid gap-4 my-6",2===n&&"md:grid-cols-2",3===n&&"md:grid-cols-3"),children:t})}}},function(e){e.O(0,[5293,2150,2716,2971,8069,1744],function(){return e(e.s=1893)}),_N_E=e.O()}]);