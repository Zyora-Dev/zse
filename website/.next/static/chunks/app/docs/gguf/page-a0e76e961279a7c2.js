(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[4761],{5621:function(e,t,s){Promise.resolve().then(s.bind(s,3158))},3158:function(e,t,s){"use strict";s.r(t),s.d(t,{default:function(){return d}});var i=s(3827),a=s(9676),l=s(5558),r=s(4657),n=s(4987);let o=[{id:"overview",title:"Overview",level:2},{id:"supported-formats",title:"Supported Formats",level:2},{id:"installation",title:"Installation",level:2},{id:"usage",title:"Usage",level:2},{id:"gpu-offloading",title:"GPU Offloading",level:2},{id:"zse-vs-gguf",title:".zse vs GGUF",level:2}];function d(){return(0,i.jsxs)("div",{className:"flex",children:[(0,i.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[(0,i.jsx)(a.lv,{title:"GGUF Compatibility",description:"Run GGUF models via llama.cpp backend for compatibility with existing model files.",badge:"Feature"}),(0,i.jsxs)(a.Je,{id:"overview",title:"Overview",children:[(0,i.jsx)("p",{className:"mb-4",children:"ZSE includes full support for GGUF models via the llama-cpp-python backend. This lets you run existing GGUF models from Hugging Face or other sources while maintaining API compatibility with the rest of ZSE."}),(0,i.jsxs)(n.gy,{columns:3,children:[(0,i.jsx)(n.Zb,{title:"GGUF v2/v3",description:"Full support for modern GGUF format versions"}),(0,i.jsx)(n.Zb,{title:"All Quant Types",description:"Q4_K_M, Q5_K_M, Q8_0, and more"}),(0,i.jsx)(n.Zb,{title:"GPU Offloading",description:"Configurable layer offloading to GPU"})]}),(0,i.jsx)(n.VS,{features:["Parse GGUF v2/v3 format metadata","Support all GGML quantization types","Streaming and non-streaming generation","Chat completion support","GPU layer offloading configuration","Seamless integration with ZSE server"]})]}),(0,i.jsxs)(a.Je,{id:"supported-formats",title:"Supported Formats",children:[(0,i.jsx)("p",{className:"mb-4",children:"ZSE supports all standard GGML quantization types found in GGUF files:"}),(0,i.jsx)("div",{className:"overflow-x-auto",children:(0,i.jsxs)("table",{className:"w-full text-sm",children:[(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{className:"border-b border-white/10",children:[(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Format"}),(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Bits"}),(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Use Case"})]})}),(0,i.jsxs)("tbody",{children:[(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 font-mono text-lime",children:"Q4_K_M"}),(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"4-bit"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"Best balance of size/quality"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 font-mono text-lime",children:"Q5_K_M"}),(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"5-bit"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"Higher quality, slightly larger"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 font-mono text-lime",children:"Q8_0"}),(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"8-bit"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"Near-lossless quality"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 font-mono text-lime",children:"Q2_K"}),(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"2-bit"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"Maximum compression"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 font-mono text-lime",children:"Q6_K"}),(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"6-bit"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"High quality"})]})]})]})})]}),(0,i.jsxs)(a.Je,{id:"installation",title:"Installation",children:[(0,i.jsx)(a.KU,{id:"cpu-only",title:"CPU Only",children:(0,i.jsx)(l.d,{language:"bash",code:"pip install llama-cpp-python"})}),(0,i.jsx)(a.KU,{id:"with-cuda",title:"With CUDA (Recommended)",children:(0,i.jsx)(l.d,{language:"bash",code:'CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python'})}),(0,i.jsx)(a.KU,{id:"with-metal",title:"With Metal (macOS)",children:(0,i.jsx)(l.d,{language:"bash",code:'CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python'})}),(0,i.jsx)(r.U,{type:"info",children:"GPU acceleration is highly recommended. CPU-only inference is 10-50x slower."})]}),(0,i.jsxs)(a.Je,{id:"usage",title:"Usage",children:[(0,i.jsx)(a.KU,{id:"python-api",title:"Python API",children:(0,i.jsx)(l.d,{language:"python",code:'from zse.gguf import GGUFWrapper, is_gguf_file\n\n# Check if file is GGUF format\nif is_gguf_file("model-Q4_K_M.gguf"):\n    # Create wrapper (matches IntelligenceOrchestrator API)\n    wrapper = GGUFWrapper("model-Q4_K_M.gguf")\n    wrapper.load()\n    \n    # Streaming generation\n    for text in wrapper.generate("Hello, how are you?"):\n        print(text, end="")\n\n    # Chat completion\n    response = wrapper.chat([\n        {"role": "user", "content": "Write a haiku about coding"}\n    ])\n    print(response)'})}),(0,i.jsx)(a.KU,{id:"cli",title:"CLI",children:(0,i.jsx)(l.d,{language:"bash",code:'# Auto-detect GGUF format and serve\nzse serve model-Q4_K_M.gguf\n\n# Show GGUF metadata\nzse info model-Q4_K_M.gguf\n\n# Run inference directly\nzse infer model-Q4_K_M.gguf --prompt "Hello, world!"'})}),(0,i.jsx)(a.KU,{id:"reading-metadata",title:"Reading Metadata",children:(0,i.jsx)(l.d,{language:"python",code:'from zse.gguf import GGUFReader\n\nreader = GGUFReader("model-Q4_K_M.gguf")\nmetadata = reader.read_metadata()\n\nprint(f"Architecture: {metadata[\'architecture\']}")\nprint(f"Context Length: {metadata[\'context_length\']}")\nprint(f"Layers: {metadata[\'num_layers\']}")\nprint(f"Quantization: {metadata[\'quantization_type\']}")'})})]}),(0,i.jsxs)(a.Je,{id:"gpu-offloading",title:"GPU Offloading",children:[(0,i.jsx)("p",{className:"mb-4",children:"Configure how many layers to offload to GPU for faster inference:"}),(0,i.jsx)(l.d,{language:"python",code:'from zse.gguf import GGUFWrapper\n\n# Offload all layers to GPU (fastest, requires most VRAM)\nwrapper = GGUFWrapper("model.gguf", n_gpu_layers=-1)\n\n# Offload first 20 layers to GPU\nwrapper = GGUFWrapper("model.gguf", n_gpu_layers=20)\n\n# CPU only (no GPU offloading)\nwrapper = GGUFWrapper("model.gguf", n_gpu_layers=0)\n\n# Auto-detect optimal layers based on available VRAM\nwrapper = GGUFWrapper("model.gguf")  # Default behavior'}),(0,i.jsx)(r.U,{type:"warning",children:"The more layers offloaded to GPU, the faster inference will be. However, you need sufficient VRAM. If you run out of memory, reduce n_gpu_layers."})]}),(0,i.jsxs)(a.Je,{id:"zse-vs-gguf",title:".zse vs GGUF",children:[(0,i.jsxs)("p",{className:"mb-4",children:["While GGUF is widely supported, native ",(0,i.jsx)(l.Z,{children:".zse"})," format offers significant advantages:"]}),(0,i.jsx)("div",{className:"overflow-x-auto",children:(0,i.jsxs)("table",{className:"w-full text-sm",children:[(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{className:"border-b border-white/10",children:[(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Feature"}),(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:".zse"}),(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"GGUF"})]})}),(0,i.jsxs)("tbody",{children:[(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"Memory Allocation"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"Streaming (on-demand)"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"Static (all at once)"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"Cold Start"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"3.9s (7B model)"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"10-30s typical"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"Memory Efficiency"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"Load only needed layers"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"Full model in RAM+VRAM"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"Quantization"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"INT4 @ full precision"}),(0,i.jsx)("td",{className:"py-3 px-4 text-gray-400",children:"Various (Q4_K_M, etc.)"})]})]})]})}),(0,i.jsx)(r.U,{type:"info",children:"Use GGUF for compatibility with existing model files. Convert to .zse for optimal performance with ZSE's streaming inference engine."}),(0,i.jsx)(l.d,{language:"bash",code:"# Convert GGUF to .zse for better performance\nzse convert model-Q4_K_M.gguf -o model.zse"})]}),(0,i.jsx)(a.KO,{prev:{href:"/docs/multi-gpu",title:"Multi-GPU"},next:{href:"/docs/api/cli",title:"CLI Commands"}})]}),(0,i.jsx)(a.o5,{items:o})]})}},4987:function(e,t,s){"use strict";s.d(t,{Rg:function(){return n},VS:function(){return o},Zb:function(){return d},gy:function(){return c}});var i=s(3827),a=s(5293),l=s(9259),r=s(2169);function n(e){let{steps:t}=e;return(0,i.jsx)("div",{className:"my-6 space-y-0",children:t.map((e,s)=>(0,i.jsxs)(a.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*s},className:"relative pl-8 pb-8 last:pb-0",children:[s<t.length-1&&(0,i.jsx)("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),(0,i.jsx)("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:(0,i.jsx)("span",{className:"text-xs font-bold text-lime",children:s+1})}),(0,i.jsxs)("div",{children:[(0,i.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:e.title}),e.description&&(0,i.jsx)("p",{className:"text-sm text-white/50 mb-3",children:e.description}),e.code&&(0,i.jsx)("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:(0,i.jsx)("code",{className:"text-sm text-lime/90 font-mono",children:e.code})}),e.content&&(0,i.jsx)("div",{className:"text-sm text-white/70",children:e.content})]})]},s))})}function o(e){let{features:t}=e;return(0,i.jsx)("ul",{className:"my-4 space-y-2",children:t.map((e,t)=>(0,i.jsxs)(a.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*t},className:"flex items-start gap-2",children:[(0,i.jsx)(l.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),(0,i.jsx)("span",{className:"text-sm text-white/70",children:e})]},t))})}function d(e){let{title:t,description:s,icon:l,href:n,children:o}=e,d=n?"a":"div";return(0,i.jsx)(a.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:n?{y:-2}:void 0,children:(0,i.jsxs)(d,{...n?{href:n,className:"block"}:{},className:(0,r.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",n&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[l&&(0,i.jsx)("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:(0,i.jsx)(l,{className:"w-4 h-4 text-lime"})}),(0,i.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:t}),s&&(0,i.jsx)("p",{className:"text-sm text-white/50",children:s}),o]})})}function c(e){let{children:t,columns:s=2}=e;return(0,i.jsx)("div",{className:(0,r.cn)("grid gap-4 my-6",2===s&&"md:grid-cols-2",3===s&&"md:grid-cols-3"),children:t})}}},function(e){e.O(0,[5293,2150,2716,2971,8069,1744],function(){return e(e.s=5621)}),_N_E=e.O()}]);