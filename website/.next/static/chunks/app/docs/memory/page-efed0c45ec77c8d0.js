(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[3524],{424:function(e,t,s){Promise.resolve().then(s.bind(s,6219))},6219:function(e,t,s){"use strict";s.r(t),s.d(t,{default:function(){return c}});var i=s(3827),n=s(9676),r=s(5558),l=s(4657),a=s(4987);let o=[{id:"overview",title:"Overview",level:2},{id:"gpu-memory",title:"GPU Memory",level:2},{id:"model-sizing",title:"Model Sizing",level:2},{id:"memory-tiers",title:"Memory Tiers",level:2},{id:"optimization",title:"Optimization Tips",level:2}];function c(){return(0,i.jsxs)("div",{className:"flex",children:[(0,i.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[(0,i.jsx)(n.lv,{title:"Memory Management",description:"Understanding and optimizing memory usage for model inference.",badge:"Core Concepts"}),(0,i.jsxs)(n.Je,{id:"overview",title:"Overview",children:[(0,i.jsx)("p",{className:"mb-4",children:"Memory management is critical for running large language models. ZSE provides multiple strategies to maximize model size while minimizing resource usage."}),(0,i.jsxs)(a.gy,{columns:3,children:[(0,i.jsx)(a.Zb,{title:"4-bit Weights",description:"4x reduction in model memory"}),(0,i.jsx)(a.Zb,{title:"4-bit KV Cache",description:"4x longer context windows"}),(0,i.jsx)(a.Zb,{title:"Memory Tiers",description:"GPU → CPU → Disk overflow"})]})]}),(0,i.jsxs)(n.Je,{id:"gpu-memory",title:"GPU Memory",children:[(0,i.jsx)("p",{className:"mb-4",children:"GPU memory is used for three main components:"}),(0,i.jsx)(r.d,{language:"text",code:"Total GPU Memory\n├── Model Weights (~60-80%)\n│   └── Quantized weights + embedding layers\n├── KV Cache (~15-30%)\n│   └── Attention key-value pairs for context\n├── Activations (~5-10%)\n│   └── Intermediate computations during inference\n└── CUDA Overhead (~2-5%)\n    └── CUDA context, kernels, buffers"}),(0,i.jsxs)(n.KU,{id:"memory-formula",title:"Memory Estimation",children:[(0,i.jsx)("p",{className:"mb-2",children:"Estimate memory requirements:"}),(0,i.jsx)(r.d,{language:"text",code:"Model Memory = (Parameters \xd7 Bits per Param) / 8\n\nExamples (4-bit quantized):\n  7B params \xd7 4 bits / 8 = 3.5 GB\n 14B params \xd7 4 bits / 8 = 7.0 GB\n 70B params \xd7 4 bits / 8 = 35 GB\n\nKV Cache Memory = 2 \xd7 Layers \xd7 Hidden \xd7 Context \xd7 Bytes per KV\n\nExample (7B model, 8K context, FP16 KV):\n  2 \xd7 32 \xd7 4096 \xd7 8192 \xd7 2 = 4.3 GB"})]}),(0,i.jsxs)(n.KU,{id:"check-memory",title:"Checking Memory",children:[(0,i.jsx)(r.d,{language:"bash",code:"# Check available GPU memory\nzse hardware\n\n# Check model memory requirements\nzse info model.zse\n\n# Monitor during inference\nwatch -n1 nvidia-smi"}),(0,i.jsx)(r.d,{language:"python",code:'from zllm_zse import ZSE\n\nmodel = ZSE("qwen-7b.zse")\n\n# Get memory info\nmem = model.memory_info()\nprint(f"Model: {mem[\'model_size\'] / 1e9:.1f} GB")\nprint(f"KV Cache: {mem[\'kv_cache_size\'] / 1e9:.1f} GB")\nprint(f"Free GPU: {mem[\'gpu_free\'] / 1e9:.1f} GB")'})]})]}),(0,i.jsxs)(n.Je,{id:"model-sizing",title:"Model Sizing",children:[(0,i.jsx)("p",{className:"mb-4",children:"Choose the right model size for your hardware:"}),(0,i.jsx)("div",{className:"overflow-x-auto my-4",children:(0,i.jsxs)("table",{className:"w-full text-sm",children:[(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{className:"border-b border-white/10",children:[(0,i.jsx)("th",{className:"text-left py-2 pr-4 text-white/50 font-medium",children:"GPU VRAM"}),(0,i.jsx)("th",{className:"text-left py-2 px-4 text-white/50 font-medium",children:"Max Model (NF4)"}),(0,i.jsx)("th",{className:"text-left py-2 pl-4 text-white/50 font-medium",children:"Context (FP16 KV)"})]})}),(0,i.jsxs)("tbody",{className:"text-white/70",children:[(0,i.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,i.jsx)("td",{className:"py-2 pr-4",children:"8 GB"}),(0,i.jsx)("td",{className:"py-2 px-4",children:"7B"}),(0,i.jsx)("td",{className:"py-2 pl-4",children:"4K"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,i.jsx)("td",{className:"py-2 pr-4",children:"12 GB"}),(0,i.jsx)("td",{className:"py-2 px-4",children:"14B"}),(0,i.jsx)("td",{className:"py-2 pl-4",children:"8K"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,i.jsx)("td",{className:"py-2 pr-4",children:"16 GB"}),(0,i.jsx)("td",{className:"py-2 px-4",children:"14B"}),(0,i.jsx)("td",{className:"py-2 pl-4",children:"16K"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,i.jsx)("td",{className:"py-2 pr-4",children:"24 GB"}),(0,i.jsx)("td",{className:"py-2 px-4",children:"34B"}),(0,i.jsx)("td",{className:"py-2 pl-4",children:"32K"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,i.jsx)("td",{className:"py-2 pr-4",children:"48 GB"}),(0,i.jsx)("td",{className:"py-2 px-4",children:"70B"}),(0,i.jsx)("td",{className:"py-2 pl-4",children:"32K"})]}),(0,i.jsxs)("tr",{children:[(0,i.jsx)("td",{className:"py-2 pr-4",children:"80 GB"}),(0,i.jsx)("td",{className:"py-2 px-4",children:"70B"}),(0,i.jsx)("td",{className:"py-2 pl-4",children:"128K"})]})]})]})}),(0,i.jsx)(l.U,{type:"tip",children:"With 4-bit KV cache, you can double the context length in the table above."})]}),(0,i.jsxs)(n.Je,{id:"memory-tiers",title:"Memory Tiers",children:[(0,i.jsx)("p",{className:"mb-4",children:"ZSE supports tiered memory for running models larger than GPU memory:"}),(0,i.jsx)(r.d,{language:"text",code:"┌─────────────────────────────────────────────────────────┐\n│ Tier 1: GPU Memory (fastest)                            │\n│   - Model weights (always)                              │\n│   - Active KV cache                                     │\n│   - Current activations                                 │\n├─────────────────────────────────────────────────────────┤\n│ Tier 2: CPU Memory (slower)                             │\n│   - Overflow KV cache                                   │\n│   - Inactive model layers (offloading)                  │\n├─────────────────────────────────────────────────────────┤\n│ Tier 3: Disk (slowest)                                  │\n│   - Cold KV cache                                       │\n│   - Very long context overflow                          │\n└─────────────────────────────────────────────────────────┘"}),(0,i.jsxs)(n.KU,{id:"cpu-offloading",title:"CPU Offloading",children:[(0,i.jsx)(r.d,{language:"bash",code:"# Offload some layers to CPU\nzse serve model.zse --offload-layers 8\n\n# Auto-detect optimal offload\nzse serve model.zse --offload auto"}),(0,i.jsx)(l.U,{type:"warning",children:"CPU offloading reduces throughput by 2-5x. Use only when GPU memory is insufficient."})]}),(0,i.jsx)(n.KU,{id:"disk-cache",title:"Disk-Based KV Cache",children:(0,i.jsx)(r.d,{language:"yaml",filename:"zse.yaml",code:"kv_cache:\n  tiers:\n    - type: gpu\n      size: auto        # Use available GPU memory\n      \n    - type: cpu\n      size: 32GB        # Overflow to system RAM\n      \n    - type: disk\n      path: /tmp/zse_cache\n      size: 100GB       # For very long contexts"})})]}),(0,i.jsxs)(n.Je,{id:"optimization",title:"Optimization Tips",children:[(0,i.jsx)(a.VS,{features:["Use NF4 quantization for best quality/size ratio","Enable 4-bit KV cache for long contexts","Set max_context to your actual needs","Use prompt caching for repeated prefixes","Monitor with nvidia-smi during development"]}),(0,i.jsx)(n.KU,{id:"oom-errors",title:"Handling OOM Errors",children:(0,i.jsx)(r.d,{language:"bash",code:"# If you get CUDA OOM errors:\n\n# 1. Reduce context length\nzse serve model.zse --max-context 2048\n\n# 2. Enable KV cache compression\nzse serve model.zse --kv-quant int4\n\n# 3. Reduce batch size\nzse serve model.zse --max-batch 8\n\n# 4. Try a smaller model\nzse convert Qwen/Qwen2.5-3B-Instruct -o qwen-3b.zse"})}),(0,i.jsx)(n.KU,{id:"benchmarking",title:"Memory Benchmarking",children:(0,i.jsx)(r.d,{language:"bash",code:"# Benchmark memory usage\nzse benchmark model.zse --metric memory\n\n# Profile memory over time\nzse serve model.zse --profile-memory --profile-output memory.json"})})]}),(0,i.jsx)(n.KO,{prev:{title:"Quantization",href:"/docs/quantization"},next:{title:"zQuantize",href:"/docs/zquantize"}})]}),(0,i.jsx)(n.o5,{items:o})]})}},4987:function(e,t,s){"use strict";s.d(t,{Rg:function(){return a},VS:function(){return o},Zb:function(){return c},gy:function(){return d}});var i=s(3827),n=s(5293),r=s(9259),l=s(2169);function a(e){let{steps:t}=e;return(0,i.jsx)("div",{className:"my-6 space-y-0",children:t.map((e,s)=>(0,i.jsxs)(n.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*s},className:"relative pl-8 pb-8 last:pb-0",children:[s<t.length-1&&(0,i.jsx)("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),(0,i.jsx)("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:(0,i.jsx)("span",{className:"text-xs font-bold text-lime",children:s+1})}),(0,i.jsxs)("div",{children:[(0,i.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:e.title}),e.description&&(0,i.jsx)("p",{className:"text-sm text-white/50 mb-3",children:e.description}),e.code&&(0,i.jsx)("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:(0,i.jsx)("code",{className:"text-sm text-lime/90 font-mono",children:e.code})}),e.content&&(0,i.jsx)("div",{className:"text-sm text-white/70",children:e.content})]})]},s))})}function o(e){let{features:t}=e;return(0,i.jsx)("ul",{className:"my-4 space-y-2",children:t.map((e,t)=>(0,i.jsxs)(n.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*t},className:"flex items-start gap-2",children:[(0,i.jsx)(r.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),(0,i.jsx)("span",{className:"text-sm text-white/70",children:e})]},t))})}function c(e){let{title:t,description:s,icon:r,href:a,children:o}=e,c=a?"a":"div";return(0,i.jsx)(n.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:a?{y:-2}:void 0,children:(0,i.jsxs)(c,{...a?{href:a,className:"block"}:{},className:(0,l.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",a&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[r&&(0,i.jsx)("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:(0,i.jsx)(r,{className:"w-4 h-4 text-lime"})}),(0,i.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:t}),s&&(0,i.jsx)("p",{className:"text-sm text-white/50",children:s}),o]})})}function d(e){let{children:t,columns:s=2}=e;return(0,i.jsx)("div",{className:(0,l.cn)("grid gap-4 my-6",2===s&&"md:grid-cols-2",3===s&&"md:grid-cols-3"),children:t})}}},function(e){e.O(0,[5293,2150,2716,2971,8069,1744],function(){return e(e.s=424)}),_N_E=e.O()}]);