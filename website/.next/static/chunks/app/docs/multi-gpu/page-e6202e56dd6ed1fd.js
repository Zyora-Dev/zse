(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[7311],{329:function(e,t,s){Promise.resolve().then(s.bind(s,2961))},2961:function(e,t,s){"use strict";s.r(t),s.d(t,{default:function(){return c}});var i=s(3827),n=s(9676),r=s(5558),l=s(4657),a=s(4987);let o=[{id:"overview",title:"Overview",level:2},{id:"how-it-works",title:"How It Works",level:2},{id:"usage",title:"Usage",level:2},{id:"vram-distribution",title:"VRAM Distribution",level:2},{id:"benchmarks",title:"Benchmarks",level:2}];function c(){return(0,i.jsxs)("div",{className:"flex",children:[(0,i.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[(0,i.jsx)(n.lv,{title:"Multi-GPU Support",description:"Run larger models by distributing weights across multiple GPUs.",badge:"Feature"}),(0,i.jsxs)(n.Je,{id:"overview",title:"Overview",children:[(0,i.jsx)("p",{className:"mb-4",children:"ZSE supports multi-GPU inference, allowing you to run models that don't fit on a single GPU by automatically sharding layers across available devices."}),(0,i.jsxs)(a.gy,{columns:3,children:[(0,i.jsx)(a.Zb,{title:"Auto Detection",description:"Automatically detects available GPUs and VRAM"}),(0,i.jsx)(a.Zb,{title:"Model Sharding",description:"Distributes layers evenly across GPUs"}),(0,i.jsx)(a.Zb,{title:"VRAM Balancing",description:"Configurable memory limits per device"})]}),(0,i.jsx)(a.VS,{features:["Automatic GPU count and memory detection","HuggingFace accelerate integration","Even layer distribution across GPUs","Configurable max_memory per GPU","Support for mixed GPU configurations"]})]}),(0,i.jsxs)(n.Je,{id:"how-it-works",title:"How It Works",children:[(0,i.jsxs)("p",{className:"mb-4",children:["Multi-GPU inference works by distributing model layers across available GPUs. ZSE uses HuggingFace's ",(0,i.jsx)(r.Z,{children:'device_map="auto"'})," under the hood for automatic layer distribution."]}),(0,i.jsx)(r.d,{language:"text",code:"Model: Qwen 32B (64 layers)\nAvailable: 2x A10G (24GB each)\n\nLayer Distribution:\n┌─────────────┐    ┌─────────────┐\n│   GPU 0     │    │   GPU 1     │\n│             │    │             │\n│ Layers 0-31 │    │ Layers 32-63│\n│             │    │             │\n│ ~12GB VRAM  │    │ ~12GB VRAM  │\n└─────────────┘    └─────────────┘\n\nForward Pass:\nInput → GPU 0 → Transfer → GPU 1 → Output"}),(0,i.jsx)(l.U,{type:"info",children:"Inter-GPU communication adds some latency, but enables running models that wouldn't fit on a single GPU. The overhead is typically 5-15%."})]}),(0,i.jsxs)(n.Je,{id:"usage",title:"Usage",children:[(0,i.jsx)(n.KU,{id:"python-api",title:"Python API",children:(0,i.jsx)(r.d,{language:"python",code:'from zse.engine.orchestrator import IntelligenceOrchestrator\n\n# Auto-detect and use all available GPUs\norch = IntelligenceOrchestrator.multi_gpu("Qwen/Qwen2.5-Coder-7B-Instruct")\norch.load()  # Model automatically sharded across GPUs\n\n# Or specify which GPUs to use\norch = IntelligenceOrchestrator.multi_gpu("model_name", gpu_ids=[0, 1])\n\n# Check GPU info\ninfo = IntelligenceOrchestrator.get_gpu_info()\nprint(f"GPUs: {info[\'count\']}, Total VRAM: {info[\'total_memory\'] / 1e9:.1f} GB")'})}),(0,i.jsx)(n.KU,{id:"cli",title:"CLI",children:(0,i.jsx)(r.d,{language:"bash",code:'# Auto-detect GPUs and distribute model\nzse serve model.zse --multi-gpu\n\n# Specify GPU IDs\nzse serve model.zse --gpus 0,1,2\n\n# Set memory limits per GPU\nzse serve model.zse --multi-gpu --max-memory "0:20GB,1:20GB"'})}),(0,i.jsx)(n.KU,{id:"gpu-info",title:"Checking GPU Info",children:(0,i.jsx)(r.d,{language:"python",code:'from zse.engine.orchestrator import IntelligenceOrchestrator\n\ninfo = IntelligenceOrchestrator.get_gpu_info()\n# Returns:\n# {\n#   "count": 2,\n#   "devices": [\n#     {"id": 0, "name": "NVIDIA A10G", "memory": 24576000000},\n#     {"id": 1, "name": "NVIDIA A10G", "memory": 24576000000}\n#   ],\n#   "total_memory": 49152000000\n# }'})})]}),(0,i.jsxs)(n.Je,{id:"vram-distribution",title:"VRAM Distribution",children:[(0,i.jsx)("p",{className:"mb-4",children:"ZSE automatically balances VRAM usage across GPUs. You can also configure maximum memory per GPU for fine-grained control."}),(0,i.jsx)(r.d,{language:"python",code:'# Set max memory per GPU (useful for reserving memory for KV cache)\norch = IntelligenceOrchestrator.multi_gpu(\n    "model_name",\n    max_memory={\n        0: "20GB",  # Leave 4GB for KV cache on GPU 0\n        1: "22GB",  # Leave 2GB for KV cache on GPU 1\n    }\n)'}),(0,i.jsx)(l.U,{type:"warning",children:"Reserve some VRAM for the KV cache and CUDA kernels. Using 100% of available memory will cause out-of-memory errors during inference."})]}),(0,i.jsxs)(n.Je,{id:"benchmarks",title:"Benchmarks",children:[(0,i.jsx)("p",{className:"mb-4",children:"Verified on Modal with 2x A10G GPUs:"}),(0,i.jsx)("div",{className:"overflow-x-auto",children:(0,i.jsxs)("table",{className:"w-full text-sm",children:[(0,i.jsx)("thead",{children:(0,i.jsxs)("tr",{className:"border-b border-white/10",children:[(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Test"}),(0,i.jsx)("th",{className:"text-left py-3 px-4 font-medium text-gray-400",children:"Result"})]})}),(0,i.jsxs)("tbody",{children:[(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"GPU Detection"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"2 GPUs detected ✓"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"Model Load (Qwen 7B FP16)"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"80s ✓"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"VRAM Distribution"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"GPU 0: 6.22 GB, GPU 1: 7.96 GB ✓"})]}),(0,i.jsxs)("tr",{className:"border-b border-white/5",children:[(0,i.jsx)("td",{className:"py-3 px-4 text-white",children:"Generation Speed"}),(0,i.jsx)("td",{className:"py-3 px-4 text-lime",children:"100 tokens @ 15.0 tok/s ✓"})]})]})]})}),(0,i.jsx)(l.U,{type:"info",children:"Multi-GPU is most beneficial for models larger than 30B parameters, where the memory savings outweigh the inter-GPU communication overhead."})]}),(0,i.jsx)(n.KO,{prev:{href:"/docs/zkv",title:"zKV"},next:{href:"/docs/gguf",title:"GGUF Compatibility"}})]}),(0,i.jsx)(n.o5,{items:o})]})}},4987:function(e,t,s){"use strict";s.d(t,{Rg:function(){return a},VS:function(){return o},Zb:function(){return c},gy:function(){return d}});var i=s(3827),n=s(5293),r=s(9259),l=s(2169);function a(e){let{steps:t}=e;return(0,i.jsx)("div",{className:"my-6 space-y-0",children:t.map((e,s)=>(0,i.jsxs)(n.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*s},className:"relative pl-8 pb-8 last:pb-0",children:[s<t.length-1&&(0,i.jsx)("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),(0,i.jsx)("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:(0,i.jsx)("span",{className:"text-xs font-bold text-lime",children:s+1})}),(0,i.jsxs)("div",{children:[(0,i.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:e.title}),e.description&&(0,i.jsx)("p",{className:"text-sm text-white/50 mb-3",children:e.description}),e.code&&(0,i.jsx)("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:(0,i.jsx)("code",{className:"text-sm text-lime/90 font-mono",children:e.code})}),e.content&&(0,i.jsx)("div",{className:"text-sm text-white/70",children:e.content})]})]},s))})}function o(e){let{features:t}=e;return(0,i.jsx)("ul",{className:"my-4 space-y-2",children:t.map((e,t)=>(0,i.jsxs)(n.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*t},className:"flex items-start gap-2",children:[(0,i.jsx)(r.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),(0,i.jsx)("span",{className:"text-sm text-white/70",children:e})]},t))})}function c(e){let{title:t,description:s,icon:r,href:a,children:o}=e,c=a?"a":"div";return(0,i.jsx)(n.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:a?{y:-2}:void 0,children:(0,i.jsxs)(c,{...a?{href:a,className:"block"}:{},className:(0,l.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",a&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[r&&(0,i.jsx)("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:(0,i.jsx)(r,{className:"w-4 h-4 text-lime"})}),(0,i.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:t}),s&&(0,i.jsx)("p",{className:"text-sm text-white/50",children:s}),o]})})}function d(e){let{children:t,columns:s=2}=e;return(0,i.jsx)("div",{className:(0,l.cn)("grid gap-4 my-6",2===s&&"md:grid-cols-2",3===s&&"md:grid-cols-3"),children:t})}}},function(e){e.O(0,[5293,2150,2716,2971,8069,1744],function(){return e(e.s=329)}),_N_E=e.O()}]);