(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[4362],{6483:function(e,t,s){Promise.resolve().then(s.bind(s,3780))},3780:function(e,t,s){"use strict";s.r(t),s.d(t,{default:function(){return o}});var n=s(3827),l=s(9676),r=s(5558),i=s(4657),a=s(4987);let c=[{id:"overview",title:"Overview",level:2},{id:"cli-usage",title:"CLI Usage",level:2},{id:"python-api",title:"Python API",level:2},{id:"parameters",title:"Parameters",level:2},{id:"chat-templates",title:"Chat Templates",level:2},{id:"batch-inference",title:"Batch Inference",level:2}];function o(){return(0,n.jsxs)("div",{className:"flex",children:[(0,n.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[(0,n.jsx)(l.lv,{title:"zInfer",description:"High-performance local inference for transformer models with optimized sampling.",badge:"Feature"}),(0,n.jsxs)(l.Je,{id:"overview",title:"Overview",children:[(0,n.jsxs)("p",{className:"mb-4",children:[(0,n.jsx)(r.Z,{children:"zInfer"})," provides direct inference capabilities for both interactive chat and programmatic text generation."]}),(0,n.jsxs)(a.gy,{columns:3,children:[(0,n.jsx)(a.Zb,{title:"~100 tok/s",description:"High throughput on consumer GPUs"}),(0,n.jsx)(a.Zb,{title:"Flash Attention",description:"Memory-efficient attention"}),(0,n.jsx)(a.Zb,{title:"Speculative",description:"2-3x faster with draft models"})]}),(0,n.jsx)(a.VS,{features:["Optimized CUDA kernels for inference","Flash Attention 2 support","Speculative decoding with draft models","Continuous batching for throughput","Custom sampling strategies"]})]}),(0,n.jsxs)(l.Je,{id:"cli-usage",title:"CLI Usage",children:[(0,n.jsxs)(l.KU,{id:"chat",title:"Interactive Chat",children:[(0,n.jsx)(r.d,{language:"bash",code:'# Start interactive chat\nzse chat qwen-7b.zse\n\n# With system prompt\nzse chat qwen-7b.zse --system "You are a helpful coding assistant"\n\n# With initial prompt\nzse chat qwen-7b.zse -p "Explain quantum computing"'}),(0,n.jsx)("p",{className:"mt-4 mb-2",children:"Chat commands:"}),(0,n.jsx)("div",{className:"overflow-x-auto my-4",children:(0,n.jsxs)("table",{className:"w-full text-sm",children:[(0,n.jsx)("thead",{children:(0,n.jsxs)("tr",{className:"border-b border-white/10",children:[(0,n.jsx)("th",{className:"text-left py-2 pr-4 text-white/50 font-medium",children:"Command"}),(0,n.jsx)("th",{className:"text-left py-2 pl-4 text-white/50 font-medium",children:"Description"})]})}),(0,n.jsxs)("tbody",{className:"text-white/70",children:[(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"/clear"})}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Clear conversation history"})]}),(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"/system <prompt>"})}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Set system prompt"})]}),(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"/temp <value>"})}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Set temperature (0.0-2.0)"})]}),(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"/save <file>"})}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Save conversation to file"})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"/quit"})}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Exit chat"})]})]})]})})]}),(0,n.jsx)(l.KU,{id:"completion",title:"Text Completion",children:(0,n.jsx)(r.d,{language:"bash",code:'# Single completion\nzse complete qwen-7b.zse -p "The quick brown fox"\n\n# With parameters\nzse complete qwen-7b.zse \\\n  -p "Write a poem about AI" \\\n  --max-tokens 200 \\\n  --temperature 0.8'})})]}),(0,n.jsxs)(l.Je,{id:"python-api",title:"Python API",children:[(0,n.jsx)(l.KU,{id:"quick-inference",title:"Quick Inference",children:(0,n.jsx)(r.d,{language:"python",code:'from zllm_zse import ZSE\n\n# Load model\nmodel = ZSE("qwen-7b.zse")\n\n# Chat completion\nresponse = model.chat([\n    {"role": "user", "content": "Hello!"}\n])\nprint(response)\n\n# Text completion\ntext = model.complete("The meaning of life is")\nprint(text)'})}),(0,n.jsx)(l.KU,{id:"streaming",title:"Streaming",children:(0,n.jsx)(r.d,{language:"python",code:'from zllm_zse import ZSE\n\nmodel = ZSE("qwen-7b.zse")\n\n# Stream chat response\nfor chunk in model.chat_stream([\n    {"role": "user", "content": "Tell me a story"}\n]):\n    print(chunk, end="", flush=True)\n\n# Stream completion\nfor token in model.complete_stream("Once upon a time"):\n    print(token, end="", flush=True)'})}),(0,n.jsx)(l.KU,{id:"async",title:"Async API",children:(0,n.jsx)(r.d,{language:"python",code:'import asyncio\nfrom zllm_zse import AsyncZSE\n\nasync def main():\n    model = AsyncZSE("qwen-7b.zse")\n    \n    # Async chat\n    response = await model.chat([\n        {"role": "user", "content": "Hello!"}\n    ])\n    \n    # Async streaming\n    async for chunk in model.chat_stream([\n        {"role": "user", "content": "Tell me a story"}\n    ]):\n        print(chunk, end="")\n\nasyncio.run(main())'})})]}),(0,n.jsxs)(l.Je,{id:"parameters",title:"Parameters",children:[(0,n.jsx)("div",{className:"overflow-x-auto my-4",children:(0,n.jsxs)("table",{className:"w-full text-sm",children:[(0,n.jsx)("thead",{children:(0,n.jsxs)("tr",{className:"border-b border-white/10",children:[(0,n.jsx)("th",{className:"text-left py-2 pr-4 text-white/50 font-medium",children:"Parameter"}),(0,n.jsx)("th",{className:"text-left py-2 px-4 text-white/50 font-medium",children:"Default"}),(0,n.jsx)("th",{className:"text-left py-2 pl-4 text-white/50 font-medium",children:"Description"})]})}),(0,n.jsxs)("tbody",{className:"text-white/70",children:[(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"temperature"})}),(0,n.jsx)("td",{className:"py-2 px-4",children:"0.7"}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Sampling randomness (0.0-2.0)"})]}),(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"top_p"})}),(0,n.jsx)("td",{className:"py-2 px-4",children:"0.9"}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Nucleus sampling threshold"})]}),(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"top_k"})}),(0,n.jsx)("td",{className:"py-2 px-4",children:"50"}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Top-k sampling (0 = disabled)"})]}),(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"max_tokens"})}),(0,n.jsx)("td",{className:"py-2 px-4",children:"2048"}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Maximum tokens to generate"})]}),(0,n.jsxs)("tr",{className:"border-b border-white/[0.05]",children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"repetition_penalty"})}),(0,n.jsx)("td",{className:"py-2 px-4",children:"1.0"}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Penalty for repeated tokens"})]}),(0,n.jsxs)("tr",{children:[(0,n.jsx)("td",{className:"py-2 pr-4",children:(0,n.jsx)(r.Z,{children:"stop"})}),(0,n.jsx)("td",{className:"py-2 px-4",children:"[]"}),(0,n.jsx)("td",{className:"py-2 pl-4",children:"Stop sequences"})]})]})]})}),(0,n.jsx)(r.d,{language:"python",code:'# Custom parameters\nresponse = model.chat(\n    messages=[{"role": "user", "content": "Write code"}],\n    temperature=0.2,      # Lower for code\n    top_p=0.95,\n    max_tokens=1000,\n    stop=["```"]       # Stop at code block end\n)'})]}),(0,n.jsxs)(l.Je,{id:"chat-templates",title:"Chat Templates",children:[(0,n.jsx)("p",{className:"mb-4",children:"ZSE automatically detects and applies the correct chat template for each model. Override if needed:"}),(0,n.jsx)(r.d,{language:"python",code:'from zllm_zse import ZSE\n\n# Use built-in template\nmodel = ZSE("qwen-7b.zse")  # Auto-detects Qwen template\n\n# Override template\nmodel = ZSE("custom-model.zse", chat_template="chatml")\n\n# Custom template string\nmodel = ZSE("model.zse", chat_template="""\n{%- for message in messages %}\n{%- if message[\'role\'] == \'user\' %}\nUser: {{ message[\'content\'] }}\n{%- elif message[\'role\'] == \'assistant\' %}\nAssistant: {{ message[\'content\'] }}\n{%- endif %}\n{%- endfor %}\nAssistant:""")'}),(0,n.jsx)("p",{className:"mt-4 mb-2",children:"Built-in templates:"}),(0,n.jsx)("div",{className:"flex flex-wrap gap-2 mb-4",children:["chatml","llama","mistral","vicuna","alpaca","zephyr"].map(e=>(0,n.jsx)("span",{className:"px-2 py-0.5 bg-white/5 rounded text-sm text-white/60",children:e},e))})]}),(0,n.jsxs)(l.Je,{id:"batch-inference",title:"Batch Inference",children:[(0,n.jsx)("p",{className:"mb-4",children:"Process multiple prompts efficiently:"}),(0,n.jsx)(r.d,{language:"python",code:'from zllm_zse import ZSE\n\nmodel = ZSE("qwen-7b.zse")\n\n# Batch completion\nprompts = [\n    "Translate to French: Hello",\n    "Translate to French: Goodbye", \n    "Translate to French: Thank you",\n]\n\nresults = model.complete_batch(prompts)\nfor prompt, result in zip(prompts, results):\n    print(f"{prompt} -> {result}")\n\n# Batch chat\nconversations = [\n    [{"role": "user", "content": "What is 2+2?"}],\n    [{"role": "user", "content": "What is 3+3?"}],\n    [{"role": "user", "content": "What is 4+4?"}],\n]\n\nresponses = model.chat_batch(conversations)\nfor response in responses:\n    print(response)'}),(0,n.jsx)(i.U,{type:"tip",children:"Batch inference can be 2-4x faster than sequential inference due to GPU parallelism."})]}),(0,n.jsx)(l.KO,{prev:{title:"zServe",href:"/docs/zserve"},next:{title:"zStream",href:"/docs/zstream"}})]}),(0,n.jsx)(l.o5,{items:c})]})}},4987:function(e,t,s){"use strict";s.d(t,{Rg:function(){return a},VS:function(){return c},Zb:function(){return o},gy:function(){return d}});var n=s(3827),l=s(5293),r=s(9259),i=s(2169);function a(e){let{steps:t}=e;return(0,n.jsx)("div",{className:"my-6 space-y-0",children:t.map((e,s)=>(0,n.jsxs)(l.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*s},className:"relative pl-8 pb-8 last:pb-0",children:[s<t.length-1&&(0,n.jsx)("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),(0,n.jsx)("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:(0,n.jsx)("span",{className:"text-xs font-bold text-lime",children:s+1})}),(0,n.jsxs)("div",{children:[(0,n.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:e.title}),e.description&&(0,n.jsx)("p",{className:"text-sm text-white/50 mb-3",children:e.description}),e.code&&(0,n.jsx)("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:(0,n.jsx)("code",{className:"text-sm text-lime/90 font-mono",children:e.code})}),e.content&&(0,n.jsx)("div",{className:"text-sm text-white/70",children:e.content})]})]},s))})}function c(e){let{features:t}=e;return(0,n.jsx)("ul",{className:"my-4 space-y-2",children:t.map((e,t)=>(0,n.jsxs)(l.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*t},className:"flex items-start gap-2",children:[(0,n.jsx)(r.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),(0,n.jsx)("span",{className:"text-sm text-white/70",children:e})]},t))})}function o(e){let{title:t,description:s,icon:r,href:a,children:c}=e,o=a?"a":"div";return(0,n.jsx)(l.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:a?{y:-2}:void 0,children:(0,n.jsxs)(o,{...a?{href:a,className:"block"}:{},className:(0,i.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",a&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[r&&(0,n.jsx)("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:(0,n.jsx)(r,{className:"w-4 h-4 text-lime"})}),(0,n.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:t}),s&&(0,n.jsx)("p",{className:"text-sm text-white/50",children:s}),c]})})}function d(e){let{children:t,columns:s=2}=e;return(0,n.jsx)("div",{className:(0,i.cn)("grid gap-4 my-6",2===s&&"md:grid-cols-2",3===s&&"md:grid-cols-3"),children:t})}}},function(e){e.O(0,[5293,2150,2716,2971,8069,1744],function(){return e(e.s=6483)}),_N_E=e.O()}]);