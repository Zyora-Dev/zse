(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[138],{1153:function(e,n,s){Promise.resolve().then(s.bind(s,41))},41:function(e,n,s){"use strict";s.r(n),s.d(n,{default:function(){return c}});var t=s(3827),i=s(9676),l=s(5558),o=s(4657),a=s(4987);let r=[{id:"overview",title:"Overview",level:2},{id:"basic-usage",title:"Basic Usage",level:2},{id:"configuration",title:"Configuration",level:2},{id:"multi-model",title:"Multi-Model Serving",level:2},{id:"scaling",title:"Scaling",level:2},{id:"monitoring",title:"Monitoring",level:2}];function c(){return(0,t.jsxs)("div",{className:"flex",children:[(0,t.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[(0,t.jsx)(i.lv,{title:"zServe",description:"Production-ready inference server with OpenAI-compatible API and sub-4-second cold start.",badge:"Feature"}),(0,t.jsxs)(i.Je,{id:"overview",title:"Overview",children:[(0,t.jsxs)("p",{className:"mb-4",children:[(0,t.jsx)(l.Z,{children:"zServe"})," is ZSE's high-performance inference server, designed for production workloads with minimal latency and maximum throughput."]}),(0,t.jsxs)(a.gy,{columns:3,children:[(0,t.jsx)(a.Zb,{title:"3.9s Cold Start",description:"Fastest model loading in the industry"}),(0,t.jsx)(a.Zb,{title:"OpenAI Compatible",description:"Drop-in replacement for OpenAI API"}),(0,t.jsx)(a.Zb,{title:"Multi-GPU",description:"Automatic model parallelism"})]}),(0,t.jsx)(a.VS,{features:["OpenAI-compatible /v1/chat/completions endpoint","Streaming responses with SSE","Request batching and queuing","Built-in rate limiting","Prometheus metrics endpoint","Health checks and graceful shutdown"]})]}),(0,t.jsxs)(i.Je,{id:"basic-usage",title:"Basic Usage",children:[(0,t.jsx)(a.Rg,{steps:[{title:"Start the server",description:"Launch with a model",code:"zse serve qwen-7b.zse"},{title:"Test the endpoint",description:"Send a request with curl",code:'curl http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{"model": "qwen-7b", "messages": [{"role": "user", "content": "Hello!"}]}\''},{title:"Use with OpenAI client",description:"Point your existing code to the server",code:'from openai import OpenAI\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="not-needed")\nresponse = client.chat.completions.create(\n    model="qwen-7b",\n    messages=[{"role": "user", "content": "Hello!"}]\n)'}]}),(0,t.jsxs)(o.U,{type:"info",children:["The server starts on port 8000 by default. Use ",(0,t.jsx)(l.Z,{children:"--port"})," to change."]})]}),(0,t.jsxs)(i.Je,{id:"configuration",title:"Configuration",children:[(0,t.jsx)(i.KU,{id:"cli-options",title:"CLI Options",children:(0,t.jsx)(l.d,{language:"bash",code:'zse serve model.zse \\\n  --port 8000           # Server port\n  --host 0.0.0.0        # Bind address\n  --workers 4           # Worker processes\n  --max-batch 32        # Max batch size\n  --max-concurrent 100  # Max concurrent requests\n  --timeout 60          # Request timeout (seconds)\n  --api-key "sk-xxx"    # Require API key'})}),(0,t.jsxs)(i.KU,{id:"config-file",title:"Config File",children:[(0,t.jsx)("p",{className:"mb-2",children:"Use a YAML config file for complex setups:"}),(0,t.jsx)(l.d,{language:"yaml",filename:"zse.yaml",code:"server:\n  host: 0.0.0.0\n  port: 8000\n  workers: 4\n  \nmodel:\n  path: ./qwen-7b.zse\n  max_batch_size: 32\n  max_sequence_length: 4096\n  \ninference:\n  temperature: 0.7\n  top_p: 0.9\n  max_tokens: 2048\n  \nlimits:\n  max_concurrent_requests: 100\n  requests_per_minute: 1000\n  timeout_seconds: 60\n  \nauth:\n  api_keys:\n    - sk-key-1\n    - sk-key-2"}),(0,t.jsx)(l.d,{language:"bash",code:"zse serve --config zse.yaml"})]}),(0,t.jsx)(i.KU,{id:"environment",title:"Environment Variables",children:(0,t.jsx)(l.d,{language:"bash",code:'# Model configuration\nexport ZSE_MODEL_PATH="./qwen-7b.zse"\nexport ZSE_MAX_BATCH_SIZE=32\n\n# Server configuration\nexport ZSE_PORT=8000\nexport ZSE_HOST=0.0.0.0\n\n# Auth\nexport ZSE_API_KEY="sk-xxx"\n\n# Start server\nzse serve'})})]}),(0,t.jsxs)(i.Je,{id:"multi-model",title:"Multi-Model Serving",children:[(0,t.jsx)("p",{className:"mb-4",children:"Serve multiple models from a single server instance:"}),(0,t.jsx)(l.d,{language:"bash",code:"# Serve multiple models from a directory\nzse serve ./models/\n\n# Or specify individually\nzse serve model1.zse model2.zse model3.zse"}),(0,t.jsx)("p",{className:"mt-4",children:"Models are loaded on-demand with LRU caching:"}),(0,t.jsx)(l.d,{language:"yaml",filename:"zse.yaml",code:"models:\n  - name: qwen-7b\n    path: ./qwen-7b.zse\n    max_loaded: true      # Keep loaded\n    \n  - name: llama-8b\n    path: ./llama-8b.zse\n    max_loaded: false     # Load on demand\n    \n  - name: codellama-34b\n    path: ./codellama-34b.zse\n    gpu: [0, 1]           # Specific GPUs\n\ncache:\n  max_models: 3           # Max models in memory\n  eviction: lru           # Eviction policy"}),(0,t.jsxs)(o.U,{type:"tip",children:["Pin frequently-used models with ",(0,t.jsx)(l.Z,{children:"max_loaded: true"})," to avoid cold starts."]})]}),(0,t.jsxs)(i.Je,{id:"scaling",title:"Scaling",children:[(0,t.jsxs)(i.KU,{id:"multi-gpu",title:"Multi-GPU",children:[(0,t.jsx)("p",{className:"mb-2",children:"Automatically shard large models across GPUs:"}),(0,t.jsx)(l.d,{language:"bash",code:"# Auto-detect and use all GPUs\nzse serve model.zse --tensor-parallel auto\n\n# Specify GPUs\nzse serve model.zse --tensor-parallel 4 --gpus 0,1,2,3\n\n# Pipeline parallelism for very large models\nzse serve model.zse --pipeline-parallel 2"})]}),(0,t.jsxs)(i.KU,{id:"load-balancing",title:"Load Balancing",children:[(0,t.jsx)("p",{className:"mb-2",children:"Run multiple instances behind a load balancer:"}),(0,t.jsx)(l.d,{language:"nginx",filename:"nginx.conf",code:'upstream zse {\n    least_conn;\n    server 127.0.0.1:8000;\n    server 127.0.0.1:8001;\n    server 127.0.0.1:8002;\n}\n\nserver {\n    listen 80;\n    \n    location /v1/ {\n        proxy_pass http://zse;\n        proxy_http_version 1.1;\n        proxy_set_header Connection "";\n        \n        # For streaming\n        proxy_buffering off;\n        proxy_cache off;\n    }\n}'})]}),(0,t.jsx)(i.KU,{id:"kubernetes",title:"Kubernetes",children:(0,t.jsx)(l.d,{language:"yaml",filename:"deployment.yaml",code:'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: zse\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: zse\n  template:\n    spec:\n      containers:\n      - name: zse\n        image: zllm/zse:latest\n        command: ["zse", "serve", "/models/qwen-7b.zse"]\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        volumeMounts:\n        - name: models\n          mountPath: /models\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000'})})]}),(0,t.jsxs)(i.Je,{id:"monitoring",title:"Monitoring",children:[(0,t.jsxs)(i.KU,{id:"health-endpoint",title:"Health Endpoint",children:[(0,t.jsx)(l.d,{language:"bash",code:"curl http://localhost:8000/health"}),(0,t.jsx)(l.d,{language:"json",code:'{\n  "status": "healthy",\n  "model": "qwen-7b",\n  "uptime": 3600,\n  "requests_processed": 15420,\n  "gpu_memory_used": "4.2 GB",\n  "gpu_memory_total": "24 GB"\n}'})]}),(0,t.jsxs)(i.KU,{id:"metrics",title:"Prometheus Metrics",children:[(0,t.jsx)(l.d,{language:"bash",code:"curl http://localhost:8000/metrics"}),(0,t.jsx)(l.d,{language:"text",code:'# TYPE zse_requests_total counter\nzse_requests_total{model="qwen-7b",status="success"} 15420\nzse_requests_total{model="qwen-7b",status="error"} 12\n\n# TYPE zse_request_duration_seconds histogram\nzse_request_duration_seconds_bucket{le="0.1"} 1000\nzse_request_duration_seconds_bucket{le="0.5"} 12000\nzse_request_duration_seconds_bucket{le="1.0"} 15000\n\n# TYPE zse_tokens_generated_total counter\nzse_tokens_generated_total{model="qwen-7b"} 2456789\n\n# TYPE zse_gpu_memory_bytes gauge\nzse_gpu_memory_bytes{gpu="0"} 4500000000'})]}),(0,t.jsx)(i.KU,{id:"logging",title:"Logging",children:(0,t.jsx)(l.d,{language:"bash",code:"# Verbose logging\nzse serve model.zse --log-level debug\n\n# JSON logs for production\nzse serve model.zse --log-format json\n\n# Log to file\nzse serve model.zse --log-file /var/log/zse/server.log"})})]}),(0,t.jsx)(i.KO,{prev:{title:"zQuantize",href:"/docs/zquantize"},next:{title:"zInfer",href:"/docs/zinfer"}})]}),(0,t.jsx)(i.o5,{items:r})]})}},4987:function(e,n,s){"use strict";s.d(n,{Rg:function(){return a},VS:function(){return r},Zb:function(){return c},gy:function(){return d}});var t=s(3827),i=s(5293),l=s(9259),o=s(2169);function a(e){let{steps:n}=e;return(0,t.jsx)("div",{className:"my-6 space-y-0",children:n.map((e,s)=>(0,t.jsxs)(i.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*s},className:"relative pl-8 pb-8 last:pb-0",children:[s<n.length-1&&(0,t.jsx)("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),(0,t.jsx)("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:(0,t.jsx)("span",{className:"text-xs font-bold text-lime",children:s+1})}),(0,t.jsxs)("div",{children:[(0,t.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:e.title}),e.description&&(0,t.jsx)("p",{className:"text-sm text-white/50 mb-3",children:e.description}),e.code&&(0,t.jsx)("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:(0,t.jsx)("code",{className:"text-sm text-lime/90 font-mono",children:e.code})}),e.content&&(0,t.jsx)("div",{className:"text-sm text-white/70",children:e.content})]})]},s))})}function r(e){let{features:n}=e;return(0,t.jsx)("ul",{className:"my-4 space-y-2",children:n.map((e,n)=>(0,t.jsxs)(i.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*n},className:"flex items-start gap-2",children:[(0,t.jsx)(l.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),(0,t.jsx)("span",{className:"text-sm text-white/70",children:e})]},n))})}function c(e){let{title:n,description:s,icon:l,href:a,children:r}=e,c=a?"a":"div";return(0,t.jsx)(i.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:a?{y:-2}:void 0,children:(0,t.jsxs)(c,{...a?{href:a,className:"block"}:{},className:(0,o.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",a&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[l&&(0,t.jsx)("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:(0,t.jsx)(l,{className:"w-4 h-4 text-lime"})}),(0,t.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:n}),s&&(0,t.jsx)("p",{className:"text-sm text-white/50",children:s}),r]})})}function d(e){let{children:n,columns:s=2}=e;return(0,t.jsx)("div",{className:(0,o.cn)("grid gap-4 my-6",2===s&&"md:grid-cols-2",3===s&&"md:grid-cols-3"),children:n})}}},function(e){e.O(0,[5293,2150,2716,2971,8069,1744],function(){return e(e.s=1153)}),_N_E=e.O()}]);