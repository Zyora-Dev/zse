(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[437],{880:function(e,n,t){Promise.resolve().then(t.bind(t,6844))},6844:function(e,n,t){"use strict";t.r(n),t.d(n,{default:function(){return l}});var s=t(3827),a=t(9676),i=t(5558),o=t(4657),r=t(4987);let c=[{id:"overview",title:"Overview",level:2},{id:"server-sent-events",title:"Server-Sent Events",level:2},{id:"python-streaming",title:"Python Streaming",level:2},{id:"client-integration",title:"Client Integration",level:2},{id:"chunked-responses",title:"Chunked Responses",level:2}];function l(){return(0,s.jsxs)("div",{className:"flex",children:[(0,s.jsxs)("article",{className:"flex-1 min-w-0 py-8 px-6 lg:px-10",children:[(0,s.jsx)(a.lv,{title:"zStream",description:"Real-time token streaming for responsive AI applications with minimal time-to-first-token.",badge:"Feature"}),(0,s.jsxs)(a.Je,{id:"overview",title:"Overview",children:[(0,s.jsxs)("p",{className:"mb-4",children:[(0,s.jsx)(i.Z,{children:"zStream"})," provides real-time token-by-token streaming for chat and completion endpoints, enabling responsive user experiences."]}),(0,s.jsxs)(r.gy,{columns:3,children:[(0,s.jsx)(r.Zb,{title:"~50ms TTFT",description:"Time to first token"}),(0,s.jsx)(r.Zb,{title:"SSE",description:"Standard Server-Sent Events"}),(0,s.jsx)(r.Zb,{title:"OpenAI Compatible",description:"Same streaming format"})]}),(0,s.jsx)(r.VS,{features:["Sub-100ms time-to-first-token","OpenAI-compatible SSE format","Backpressure handling for slow clients","Graceful stream cancellation","Token usage stats in final chunk"]})]}),(0,s.jsxs)(a.Je,{id:"server-sent-events",title:"Server-Sent Events",children:[(0,s.jsxs)("p",{className:"mb-4",children:["Enable streaming with ",(0,s.jsx)(i.Z,{children:"stream: true"})," in your request:"]}),(0,s.jsx)(i.d,{language:"bash",code:'curl http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "qwen-7b",\n    "messages": [{"role": "user", "content": "Tell me a story"}],\n    "stream": true\n  }\''}),(0,s.jsxs)("p",{className:"mt-4 mb-2",children:["Response format (each line prefixed with ",(0,s.jsx)(i.Z,{children:"data: "}),"):"]}),(0,s.jsx)(i.d,{language:"json",code:'data: {"id":"chat-1","object":"chat.completion.chunk","created":1234567890,"model":"qwen-7b","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}\n\ndata: {"id":"chat-1","object":"chat.completion.chunk","created":1234567890,"model":"qwen-7b","choices":[{"index":0,"delta":{"content":"Once"},"finish_reason":null}]}\n\ndata: {"id":"chat-1","object":"chat.completion.chunk","created":1234567890,"model":"qwen-7b","choices":[{"index":0,"delta":{"content":" upon"},"finish_reason":null}]}\n\ndata: {"id":"chat-1","object":"chat.completion.chunk","created":1234567890,"model":"qwen-7b","choices":[{"index":0,"delta":{"content":" a"},"finish_reason":null}]}\n\ndata: {"id":"chat-1","object":"chat.completion.chunk","created":1234567890,"model":"qwen-7b","choices":[{"index":0,"delta":{},"finish_reason":"stop"}],"usage":{"prompt_tokens":15,"completion_tokens":42,"total_tokens":57}}\n\ndata: [DONE]'}),(0,s.jsxs)(o.U,{type:"info",children:["The final chunk includes ",(0,s.jsx)(i.Z,{children:"finish_reason"})," and",(0,s.jsx)(i.Z,{children:"usage"})," stats."]})]}),(0,s.jsxs)(a.Je,{id:"python-streaming",title:"Python Streaming",children:[(0,s.jsx)(a.KU,{id:"openai-client",title:"OpenAI Client",children:(0,s.jsx)(i.d,{language:"python",code:'from openai import OpenAI\n\nclient = OpenAI(base_url="http://localhost:8000/v1", api_key="not-needed")\n\n# Streaming chat completion\nstream = client.chat.completions.create(\n    model="qwen-7b",\n    messages=[{"role": "user", "content": "Tell me a story"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end="", flush=True)'})}),(0,s.jsx)(a.KU,{id:"native-api",title:"Native API",children:(0,s.jsx)(i.d,{language:"python",code:'from zllm_zse import ZSE\n\nmodel = ZSE("qwen-7b.zse")\n\n# Generator-based streaming\nfor token in model.chat_stream([\n    {"role": "user", "content": "Tell me a story"}\n]):\n    print(token, end="", flush=True)\n\n# With callback\ndef on_token(token: str):\n    print(token, end="", flush=True)\n\nmodel.chat(\n    messages=[{"role": "user", "content": "Tell me a story"}],\n    stream_callback=on_token\n)'})}),(0,s.jsx)(a.KU,{id:"async-streaming",title:"Async Streaming",children:(0,s.jsx)(i.d,{language:"python",code:'import asyncio\nfrom openai import AsyncOpenAI\n\nasync def stream_chat():\n    client = AsyncOpenAI(\n        base_url="http://localhost:8000/v1",\n        api_key="not-needed"\n    )\n    \n    stream = await client.chat.completions.create(\n        model="qwen-7b",\n        messages=[{"role": "user", "content": "Tell me a story"}],\n        stream=True\n    )\n    \n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end="", flush=True)\n\nasyncio.run(stream_chat())'})})]}),(0,s.jsxs)(a.Je,{id:"client-integration",title:"Client Integration",children:[(0,s.jsxs)(a.KU,{id:"javascript",title:"JavaScript/TypeScript",children:[(0,s.jsx)(i.d,{language:"typescript",code:"// Using OpenAI SDK\nimport OpenAI from 'openai';\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:8000/v1',\n  apiKey: 'not-needed',\n});\n\nasync function streamChat() {\n  const stream = await openai.chat.completions.create({\n    model: 'qwen-7b',\n    messages: [{ role: 'user', content: 'Tell me a story' }],\n    stream: true,\n  });\n\n  for await (const chunk of stream) {\n    const content = chunk.choices[0]?.delta?.content || '';\n    process.stdout.write(content);\n  }\n}"}),(0,s.jsx)(i.d,{language:"typescript",code:"// Using fetch with EventSource\nasync function streamWithFetch() {\n  const response = await fetch('http://localhost:8000/v1/chat/completions', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      model: 'qwen-7b',\n      messages: [{ role: 'user', content: 'Tell me a story' }],\n      stream: true,\n    }),\n  });\n\n  const reader = response.body!.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n    \n    const text = decoder.decode(value);\n    const lines = text.split('\\n').filter(line => line.startsWith('data: '));\n    \n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n      \n      const chunk = JSON.parse(data);\n      const content = chunk.choices[0]?.delta?.content || '';\n      process.stdout.write(content);\n    }\n  }\n}"})]}),(0,s.jsx)(a.KU,{id:"react",title:"React Integration",children:(0,s.jsx)(i.d,{language:"tsx",code:"import { useState, useCallback } from 'react';\n\nfunction ChatComponent() {\n  const [response, setResponse] = useState('');\n  const [loading, setLoading] = useState(false);\n\n  const sendMessage = useCallback(async (message: string) => {\n    setLoading(true);\n    setResponse('');\n\n    const res = await fetch('/v1/chat/completions', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({\n        model: 'qwen-7b',\n        messages: [{ role: 'user', content: message }],\n        stream: true,\n      }),\n    });\n\n    const reader = res.body!.getReader();\n    const decoder = new TextDecoder();\n\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n      \n      const text = decoder.decode(value);\n      // Parse SSE and update state\n      const matches = text.matchAll(/data: ({.*})/g);\n      for (const match of matches) {\n        const chunk = JSON.parse(match[1]);\n        const content = chunk.choices[0]?.delta?.content || '';\n        setResponse(prev => prev + content);\n      }\n    }\n\n    setLoading(false);\n  }, []);\n\n  return (\n    <div>\n      <div>{response}</div>\n      <button onClick={() => sendMessage('Hello!')}>\n        {loading ? 'Generating...' : 'Send'}\n      </button>\n    </div>\n  );\n}"})})]}),(0,s.jsxs)(a.Je,{id:"chunked-responses",title:"Chunked Responses",children:[(0,s.jsx)("p",{className:"mb-4",children:"Control how tokens are grouped in streaming responses:"}),(0,s.jsx)(i.d,{language:"bash",code:'# Stream every token (default)\ncurl ... -d \'{"stream": true}\'\n\n# Stream every N tokens\ncurl ... -d \'{"stream": true, "stream_options": {"chunk_size": 5}}\'\n\n# Stream by words (space-delimited)\ncurl ... -d \'{"stream": true, "stream_options": {"chunk_by": "word"}}\'\n\n# Stream by sentences\ncurl ... -d \'{"stream": true, "stream_options": {"chunk_by": "sentence"}}\''}),(0,s.jsx)(o.U,{type:"tip",children:"Larger chunk sizes reduce network overhead but increase perceived latency. Token-by-token streaming provides the best UX for chat applications."}),(0,s.jsx)("p",{className:"mt-4 mb-2",children:(0,s.jsx)("strong",{className:"text-white",children:"Python configuration:"})}),(0,s.jsx)(i.d,{language:"python",code:'# Stream configuration\nresponse = model.chat_stream(\n    messages=[{"role": "user", "content": "Hello"}],\n    chunk_size=1,        # Tokens per chunk\n    include_usage=True,  # Include token counts\n)'})]}),(0,s.jsx)(a.KO,{prev:{title:"zInfer",href:"/docs/zinfer"},next:{title:"zKV",href:"/docs/zkv"}})]}),(0,s.jsx)(a.o5,{items:c})]})}},4987:function(e,n,t){"use strict";t.d(n,{Rg:function(){return r},VS:function(){return c},Zb:function(){return l},gy:function(){return d}});var s=t(3827),a=t(5293),i=t(9259),o=t(2169);function r(e){let{steps:n}=e;return(0,s.jsx)("div",{className:"my-6 space-y-0",children:n.map((e,t)=>(0,s.jsxs)(a.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},transition:{delay:.1*t},className:"relative pl-8 pb-8 last:pb-0",children:[t<n.length-1&&(0,s.jsx)("div",{className:"absolute left-[11px] top-6 bottom-0 w-px bg-white/10"}),(0,s.jsx)("div",{className:"absolute left-0 top-0 w-6 h-6 rounded-full bg-lime/20 border border-lime/40 flex items-center justify-center",children:(0,s.jsx)("span",{className:"text-xs font-bold text-lime",children:t+1})}),(0,s.jsxs)("div",{children:[(0,s.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:e.title}),e.description&&(0,s.jsx)("p",{className:"text-sm text-white/50 mb-3",children:e.description}),e.code&&(0,s.jsx)("pre",{className:"bg-white/[0.03] border border-white/[0.06] rounded-lg p-3 overflow-x-auto my-2",children:(0,s.jsx)("code",{className:"text-sm text-lime/90 font-mono",children:e.code})}),e.content&&(0,s.jsx)("div",{className:"text-sm text-white/70",children:e.content})]})]},t))})}function c(e){let{features:n}=e;return(0,s.jsx)("ul",{className:"my-4 space-y-2",children:n.map((e,n)=>(0,s.jsxs)(a.E.li,{initial:{opacity:0,x:-10},animate:{opacity:1,x:0},transition:{delay:.05*n},className:"flex items-start gap-2",children:[(0,s.jsx)(i.Z,{className:"w-4 h-4 text-lime mt-0.5 flex-shrink-0"}),(0,s.jsx)("span",{className:"text-sm text-white/70",children:e})]},n))})}function l(e){let{title:n,description:t,icon:i,href:r,children:c}=e,l=r?"a":"div";return(0,s.jsx)(a.E.div,{initial:{opacity:0,y:10},animate:{opacity:1,y:0},whileHover:r?{y:-2}:void 0,children:(0,s.jsxs)(l,{...r?{href:r,className:"block"}:{},className:(0,o.cn)("p-4 rounded-lg border border-white/[0.06] bg-white/[0.02]",r&&"hover:border-lime/30 hover:bg-white/[0.04] transition-all cursor-pointer"),children:[i&&(0,s.jsx)("div",{className:"w-8 h-8 rounded-lg bg-lime/10 flex items-center justify-center mb-3",children:(0,s.jsx)(i,{className:"w-4 h-4 text-lime"})}),(0,s.jsx)("h4",{className:"text-base font-semibold text-white mb-1",children:n}),t&&(0,s.jsx)("p",{className:"text-sm text-white/50",children:t}),c]})})}function d(e){let{children:n,columns:t=2}=e;return(0,s.jsx)("div",{className:(0,o.cn)("grid gap-4 my-6",2===t&&"md:grid-cols-2",3===t&&"md:grid-cols-3"),children:n})}}},function(e){e.O(0,[5293,2150,2716,2971,8069,1744],function(){return e(e.s=880)}),_N_E=e.O()}]);